<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>WeClone — 用微信聊天记录打造自己的 AI 数字分身 | MarkBai的数字自留地</title><meta name="author" content="MarkBai,blog@051088.xyz"><meta name="copyright" content="MarkBai"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本指南演示如何在 Windows&#x2F;WSL 下使用 xming521&#x2F;WeClone 项目，创造一个拥有你“那味儿”的数字分身。">
<meta property="og:type" content="article">
<meta property="og:title" content="WeClone — 用微信聊天记录打造自己的 AI 数字分身">
<meta property="og:url" content="https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/index.html">
<meta property="og:site_name" content="MarkBai的数字自留地">
<meta property="og:description" content="本指南演示如何在 Windows&#x2F;WSL 下使用 xming521&#x2F;WeClone 项目，创造一个拥有你“那味儿”的数字分身。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-img.051088.xyz/weclone.png">
<meta property="article:published_time" content="2025-05-14T14:00:00.000Z">
<meta property="article:modified_time" content="2025-05-16T12:59:37.337Z">
<meta property="article:author" content="MarkBai">
<meta property="article:tag" content="WeClone">
<meta property="article:tag" content="数字分身">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="大语言模型">
<meta property="article:tag" content="指南">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-img.051088.xyz/weclone.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "WeClone — 用微信聊天记录打造自己的 AI 数字分身",
  "url": "https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/",
  "image": "https://blog-img.051088.xyz/weclone.png",
  "datePublished": "2025-05-14T14:00:00.000Z",
  "dateModified": "2025-05-16T12:59:37.337Z",
  "author": [
    {
      "@type": "Person",
      "name": "MarkBai",
      "url": "https://blog.051088.xyz/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="bing_site_verification" content="3D5AD97894B95C6A0545D582BD3EB688"/><meta name="baidu-site-verification" content="codeva-oo2hqM6DSL"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?daf1aae7138e285b091e5da481b059e5";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'WeClone — 用微信聊天记录打造自己的 AI 数字分身',
  isHighlightShrink: undefined,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #f9fcff;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/profile-photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(https://blog-img.051088.xyz/wallhaven-5gwvz5_1920x1080.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">MarkBai的数字自留地</span></a><a class="nav-page-title" href="/"><span class="site-name">WeClone — 用微信聊天记录打造自己的 AI 数字分身</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">WeClone — 用微信聊天记录打造自己的 AI 数字分身</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-14T14:00:00.000Z" title="发表于 2025-05-14 22:00:00">2025-05-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-16T12:59:37.337Z" title="更新于 2025-05-16 20:59:37">2025-05-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/">技术教程</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/AI%E5%BA%94%E7%94%A8/">AI应用</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="WeClone：让你的聊天记录拥有“灵魂”"><a href="#WeClone：让你的聊天记录拥有“灵魂”" class="headerlink" title="WeClone：让你的聊天记录拥有“灵魂”"></a>WeClone：让你的聊天记录拥有“灵魂”</h2><p>你是否曾设想过——如果你的聊天风格、口头禅，甚至独特的表达习惯，能够被 AI 学会，并在数字世界中“复刻”一个你，会是怎样的体验？ <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone">xming521&#x2F;WeClone 项目</a> 正是在这样一个愿景下诞生的尝试。</p>
<p>WeClone 是一个一站式解决方案，它通过分析你的微信聊天记录，微调大语言模型（LLM），让模型说出“你那味儿”的话，并将其接入聊天机器人，打造出专属于你的数字分身。</p>
<details>   <summary>点击展开查看我的训练效果图</summary>   <div style="display: flex; justify-content: center; align-items: stretch; gap: 10px; margin-top: 10px;">     <img src="https://blog-img.051088.xyz/%E6%9C%80%E7%BB%88%E6%95%88%E6%9E%9C1.png" alt="图1" style="max-width: 55%; object-fit: contain;" />     <img src="https://blog-img.051088.xyz/%E6%9C%80%E7%BB%88%E6%95%88%E6%9E%9C.png" alt="图2" style="max-width: 35%; object-fit: contain;" />   </div> </details>

<hr>
<p>然而，WeClone 官方并未对 <strong>Windows 环境进行系统测试</strong>，因此许多 Windows 用户在安装与运行过程中可能会遇到各种问题。此外，项目中提到的 <strong>QLoRA 微调流程</strong>在官方文档中讲解较为简略，对新手用户不够友好。</p>
<p>这篇博客将作为一份补充指南，专门面向 <strong>Windows 用户</strong>，介绍如何在本地从零搭建和运行 WeClone，并一步步训练出属于你自己的数字克隆。</p>
<blockquote>
<p>[!WARNING]<br> <strong>重要提示</strong>：WeClone 项目仍在快速更新迭代中，当前效果并非最终状态。本教程基于WeClone 0.2.2书写。微调结果受模型规模、数据数量及数据质量等因素影响较大。通常来说，<strong>模型越大、数据越多、表达越一致，效果越接近原始人格特征</strong>。此外，由于 Windows 环境并未作为官方推荐平台，本指南旨在提供一个实践参考，实际运行中仍可能遇到特定问题。</p>
</blockquote>
<hr>
<p>在正式开始前，请务必注意：</p>
<blockquote>
<p>⚠️ <strong>隐私第一！</strong></p>
<p>请务必妥善保护自己的聊天记录及个人信息，<strong>切勿上传至公共平台</strong>，并确保本项目不被用于任何非法用途。使用者应对自己的数据使用和行为承担全部责任。</p>
</blockquote>
<hr>
<h2 id="所需硬件配置"><a href="#所需硬件配置" class="headerlink" title="所需硬件配置"></a>所需硬件配置</h2><p>运行 WeClone（尤其是模型微调阶段）对硬件有较高要求，<strong>重点在于显存（VRAM）</strong>。不建议在集成显卡或仅使用 CPU 的环境下运行，推荐使用带有独立 GPU 的设备，或租用云端 GPU 服务。</p>
<p>项目默认使用 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct"><code>Qwen2.5-7B-Instruct</code></a> 模型，并采用 <strong>LoRA</strong> 方法进行微调，显存需求约为 <strong>16GB</strong>。</p>
<p>下表列出了不同模型规模与微调方法所需的显存估算（数据来源于 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA Factory</a>）：</p>
<table>
<thead>
<tr>
<th>微调方法</th>
<th>精度 (bits)</th>
<th>7B 模型</th>
<th>14B 模型</th>
<th>30B 模型</th>
<th>70B 模型</th>
<th><code>x</code>B 模型</th>
</tr>
</thead>
<tbody><tr>
<td>Full (<code>bf16</code> &#x2F; <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code> GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code> GB</td>
</tr>
<tr>
<td>Freeze &#x2F; LoRA &#x2F; GaLore &#x2F; APOLLO</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code> GB</td>
</tr>
</tbody></table>
<blockquote>
<p>✅ <strong>建议：</strong></p>
<ul>
<li>显存 ≥16GB：推荐使用默认的 LoRA 微调方案。</li>
<li>显存 &lt;16GB：可考虑切换至 QLoRA 或选择更小参数量的模型。</li>
</ul>
</blockquote>
<p>此外，请预留至少 <strong>20GB 以上硬盘空间</strong>，以存储模型文件、中间结果和缓存数据。</p>
<p>如果你希望启用 QLoRA 微调方式，请查阅后续章节 “<strong>修改配置文件 (<code>settings.jsonc</code>)</strong>” 了解如何切换微调策略。</p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>要在 <code>Windows</code> 上顺利运行 WeClone，我们需要先搭建好相应的环境。由于现在的<code>数据清洗</code>默认使用<code>vllm</code>加速推理，而<code>vllm</code>目前仅支持<code>Linux</code>系统，所以对<code>Windows</code>用户而言，要么使用虚拟机&#x2F;wsl2，要么选择舍弃掉<code>数据清洗</code>功能，即将配置里的<code>enable_clean</code>设为false，不清洗数据集。</p>
<h3 id="环境配置（-WSL2版）"><a href="#环境配置（-WSL2版）" class="headerlink" title="环境配置（ WSL2版）"></a>环境配置（ <code>WSL2</code>版）</h3><details>
 <summary> 环境配置（ `WSL2`版）</summary>
WSL2（Windows Subsystem for Linux 2）是 Windows 提供的一种轻量级 Linux 运行环境，具备完整的 Linux 内核，并支持更好的文件系统性能和兼容性。它允许用户在 Windows 系统中运行 Linux 命令行工具和应用程序，而无需安装虚拟机或双系统。

<h4 id="安装WSL2"><a href="#安装WSL2" class="headerlink" title="安装WSL2"></a><strong>安装<code>WSL2</code></strong></h4><p><code>WSL2</code>的安装教程互联网上有很多，这里推荐两个讲解比较详细的，可以参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011119817/article/details/130745551">windows11 安装WSL2全流程</a>（不需要安装图形界面）</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Natsuago/article/details/145594631">Win10&#x2F;11系统下WSL2+Ubuntu20.04的全流程安装指南</a></li>
</ul>
<h4 id="Git-版本控制"><a href="#Git-版本控制" class="headerlink" title="Git 版本控制"></a><strong>Git 版本控制</strong></h4><p>你需要 <code>Git</code>来克隆 WeClone 项目仓库。<br>不同的Linux平台可以选择不同的安装方式，你可以直接参考下面的这篇教程，选择适合你的方法</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.runoob.com/git/git-install-setup.html">Git 安装配置 | 菜鸟教程</a></li>
</ul>
<h4 id="创建虚拟环境并安装-WeClone-依赖"><a href="#创建虚拟环境并安装-WeClone-依赖" class="headerlink" title="创建虚拟环境并安装 WeClone 依赖"></a><strong>创建虚拟环境并安装 WeClone 依赖</strong></h4><p><code>uv</code>是一个由 Astral 开发的快速 Python 包管理器。推荐使用 <code>uv</code> 创建虚拟环境并安装项目依赖，可以避免包版本冲突。</p>
<ul>
<li><p><strong>安装 uv (推荐的包管理器)</strong><br>打开命令提示符 (CMD) 或 PowerShell，使用 pip 安装 uv：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p><strong>cuda安装</strong>(已安装可跳过，<strong>要求版本12.4及以上</strong>)：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda">LLaMA Factory</a></p>
<ul>
<li><strong>参考教程：</strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46472656/article/details/138624468">WLS2安装CUDA保姆级教程_wsl2 cuda-CSDN博客</a></li>
</ul>
</li>
<li><p><strong>克隆 WeClone 项目：</strong><br>打开 CMD 或 PowerShell，导航到你希望存放项目的目录，然后克隆仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/xming521/WeClone.git</span><br><span class="line"><span class="built_in">cd</span> WeClone</span><br></pre></td></tr></table></figure>

<p> 这里直接<code>git clone</code>可能会请求超时，例如<code>Failed to connect to github.com port 443 after 132282 ms: Connection timed out</code>。这是因为国内访问Github网络环境不稳定，需要科学上网。而<code>WSL2</code>的默认无法使用你Windows环境下的代理，因此</p>
<p> 这里需要单独为<code>WSL2 </code>设置代理。下面是一篇参考教程：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/153124468">为 WSL2 一键设置代理 - 知乎</a></li>
</ul>
</li>
<li><p><strong>创建并激活虚拟环境 (使用 uv)：</strong><br>在 <code>WeClone</code> 项目根目录下执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uv venv .venv --python=3.10  <span class="comment"># 你可以指定已安装的 Python 3.10+ 版本</span></span><br><span class="line"><span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure>

<p>激活成功后，你的命令行提示符前通常会显示 <code>(.venv)</code>。</p>
</li>
<li><p>**（直接跳过）手动安装 <code>pytorch</code> **（现在已经不需要了，WeClone的pyproject.toml默认使用了清华源）</p>
 <details>
   <summary>手动安装 PyTorch 参考教程</summary>
   <p>由于国内环境，和其他包一起安装 PyTorch 大概率会出错，所以最好先在环境内安装好 PyTorch。推荐从一些国内镜像源下载好 PyTorch 安装包后在本地离线安装。可以参考下面的教程，但是注意教程中使用的是下载官方包的链接，需要替换成国内镜像源的对应网站。</p>
   <p><strong>参考教程：</strong><a href="https://blog.csdn.net/weixin_44956153/article/details/142303905" target="_blank">PyTorch 离线版本安装教程</a></p>
 </details>

</li>
<li><p><strong>安装项目主要依赖：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install --group main -e .</span><br></pre></td></tr></table></figure>

<p>此命令将读取项目中依赖配置并安装所有库。<del>（如果项目担心重复安装torch，你可以考虑临时将其注释掉或在安装时用 pip 避免重复安装）。</del></p>
</li>
<li><p><strong>测试 CUDA 环境 (NVIDIA GPU 用户)：</strong><br>安装完依赖后（特别是 PyTorch），运行以下命令测试 CUDA 是否配置正确并能被 PyTorch 识别：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(&#x27;CUDA是否可用:&#x27;, torch.cuda.is_available()); print(&#x27;CUDA版本:&#x27;, torch.version.cuda); print(&#x27;PyTorch版本:&#x27;, torch.__version__)&quot;</span></span><br></pre></td></tr></table></figure>

<p>如果 <code>CUDA是否可用:</code> 显示 <code>True</code>，则表示配置成功。</p>
</li>
<li><p><strong>复制配置文件模板</strong></p>
<p>将配置文件模板复制一份并重命名为<code>settings.jsonc</code>，后续配置修改在此文件进行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> settings.template.jsonc settings.jsonc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>(可选) 安装 FlashAttention：</strong><br>为了加速训练和推理（如果你的硬件支持），可以尝试安装 FlashAttention。</p>
<p><strong>再次检查本地python、torch、cuda的版本</strong>（如果你很清楚你当前的配置可以不用检查）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python --version &amp;&amp;</span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.__version__); print(torch.cuda.is_available())&quot;</span> &amp;&amp;</span><br><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>

<p>执行以上命令，得到你的<code>python</code>、<code>torch</code>和<code>cuda</code>版本。正常来讲你会得到下面的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Python 3.10.12 <span class="comment">#python版本，应该是3.10</span></span><br><span class="line">2.6.0+cu124 <span class="comment">#你的torch版本，安教程来安装的应该是2.6.0</span></span><br><span class="line">True <span class="comment">#CUDA可用</span></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2024 NVIDIA Corporation</span><br><span class="line">Built on Thu_Sep_12_02:18:05_PDT_2024</span><br><span class="line">Cuda compilation tools, release 12.6, V12.6.77</span><br><span class="line">Build cuda_12.6.r12.6/compiler.34841621_0 <span class="comment">#cuda版本，应该大于12.4</span></span><br></pre></td></tr></table></figure>

<p>确定自己的相关版本后，在<a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/releases">Flash-attention下载地址</a>选择对应的<code>whl</code>文件用<code>pip install</code>来安装，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br><span class="line">pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Linux编辑文件操作教程</strong>（如果你不熟悉Linux的基本操作这可能会帮到你）</p>
<p>在后续涉及到修改文件时候，可以采用以下两种方法：</p>
<p><strong>方法1：VS Code（推荐）</strong></p>
<p>通过<strong>VS Code</strong>连接你的<code>WSL</code>，让你在<code>IDE</code>下轻松修改和运行项目文件</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-vscode">开始通过 WSL 使用 VS Code | Microsoft Learn</a></p>
<p><strong>方法2：</strong></p>
<p>如果你不想<code>VS Code</code>，可以直接采用<code>Linux</code>默认预装的命令行文本编辑器<code>nano</code>。在这个项目的使用中，你只需要记住下面几个命令就可用了。</p>
<ol>
<li><p>在根目录下运行<code>nano</code>编辑器</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nano settings.jsonc </span><br></pre></td></tr></table></figure>
</li>
<li><p>移动光标到你要修改的位置，进行对应修改操作。</p>
<p><code>Ctrl + ^</code> 是标记开始（英文叫 “set mark”）；</p>
<p><code>Alt + 6</code> 是复制（保留内容），<code>Ctrl + K</code> 是剪切（删除原内容）；<code>Ctrl + U</code> 是粘贴(仅粘贴内部复制或剪切内容)；</p>
<p><code>鼠标右键</code>是粘贴外部内容。</p>
</li>
<li><p>修改后按 <code>Ctrl + O</code> 保存，按 <code>Enter</code> 确认；再按 <code>Ctrl + X</code> 退出。</p>
</li>
</ol>
</li>
</ol>
</details>

<hr>
<h3 id="环境配置（纯-Windows-版）"><a href="#环境配置（纯-Windows-版）" class="headerlink" title="环境配置（纯 Windows 版）"></a>环境配置（纯 Windows 版）</h3><details>
 <summary> 环境配置（纯 Windows 版）</summary>

<h4 id="Git-版本控制-1"><a href="#Git-版本控制-1" class="headerlink" title="Git 版本控制"></a><strong>Git 版本控制</strong></h4><p>你需要 <code>Git</code> 来克隆 WeClone 项目仓库。<br>从 <a target="_blank" rel="noopener" href="https://gitforwindows.org/">Git for Windows</a> 下载并安装。你可以参考下面这篇教程：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42242910/article/details/136297201">windows安装git（全网最详细，保姆教程）-CSDN博客</a></li>
</ul>
<h4 id="创建虚拟环境并安装-WeClone-依赖-1"><a href="#创建虚拟环境并安装-WeClone-依赖-1" class="headerlink" title="创建虚拟环境并安装 WeClone 依赖"></a>创建虚拟环境并安装 WeClone 依赖</h4><p><code>uv</code>是一个由 Astral 开发的快速 Python 包管理器。推荐使用 <code>uv</code> 创建虚拟环境并安装项目依赖，可以避免包版本冲突。</p>
<ul>
<li><p><strong>安装 uv (推荐的包管理器)</strong><br>打开命令提示符 (CMD) 或 PowerShell，使用 pip 安装 uv：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p><strong>cuda安装</strong>(已安装可跳过，<strong>要求版本12.4及以上</strong>)：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda">LLaMA Factory</a></p>
<ul>
<li><strong>参考教程：</strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/YYDS_WV/article/details/137825313">CUDA与CUDNN在Windows下的安装与配置（超级详细版）_windows安装cudnn-CSDN博客</a></li>
</ul>
</li>
<li><p><strong>克隆 WeClone 项目：</strong><br>打开 CMD 或 PowerShell，导航到你希望存放项目的目录，然后克隆仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/xming521/WeClone.git</span><br><span class="line"><span class="built_in">cd</span> WeClone</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>创建并激活虚拟环境 (使用 uv)：</strong><br>在 <code>WeClone</code> 项目根目录下执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uv venv .venv --python=3.10  <span class="comment"># 你可以指定已安装的 Python 3.10+ 版本</span></span><br><span class="line">.venv\Scripts\activate</span><br></pre></td></tr></table></figure>
<p>激活成功后，你的命令行提示符前通常会显示 <code>(.venv)</code>。</p>
</li>
<li><p>**（直接跳过）手动安装 <code>pytorch</code> **（现在已经不需要了，WeClone的pyproject.toml默认使用了清华源）</p>
 <details>
   <summary>手动安装 PyTorch 参考教程</summary>
   <p>由于国内环境，和其他包一起安装 PyTorch 大概率会出错，所以最好先在环境内安装好 PyTorch。推荐从一些国内镜像源下载好 PyTorch 安装包后在本地离线安装。可以参考下面的教程，但是注意教程中使用的是下载官方包的链接，需要替换成国内镜像源的对应网站。</p>
   <p><strong>参考教程：</strong><a href="https://blog.csdn.net/weixin_44956153/article/details/142303905" target="_blank">PyTorch 离线版本安装教程</a></p>
 </details>
</li>
<li><p><strong>安装项目主要依赖：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install --group main -e .</span><br></pre></td></tr></table></figure>

<p>此命令将读取项目中依赖配置并安装所有库。<del>（如果项目担心重复安装torch，你可以考虑临时将其注释掉或在安装时用 pip 避免重复安装）。</del></p>
</li>
<li><p><strong>测试 CUDA 环境 (NVIDIA GPU 用户)：</strong><br>安装完依赖后（特别是 PyTorch），运行以下命令测试 CUDA 是否配置正确并能被 PyTorch 识别：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(&#x27;CUDA是否可用:&#x27;, torch.cuda.is_available()); print(&#x27;CUDA版本:&#x27;, torch.version.cuda); print(&#x27;PyTorch版本:&#x27;, torch.__version__)&quot;</span></span><br></pre></td></tr></table></figure>
<p>如果 <code>CUDA是否可用:</code> 显示 <code>True</code>，则表示配置成功。</p>
</li>
<li><p><strong>复制配置文件模板</strong></p>
<p>将配置文件模板复制一份并重命名为<code>settings.jsonc</code>，后续配置修改在此文件进行：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">copy</span> settings.template.jsonc settings.jsonc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>(可选) 安装 FlashAttention：</strong><br>为了加速训练和推理（如果你的硬件支持），可以尝试安装 FlashAttention。</p>
<p>参考教程：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21491605/article/details/136109125">Windows环境下flash-attention安装_flashattention2 windows 安装-CSDN博客</a></p>
<blockquote>
<p>[!NOTE] FlashAttention 在 Windows 上安装较复杂，且并非所有显卡支持，如遇失败可跳过，项目仍可运行，只是推理速度稍慢。</p>
</blockquote>
</li>
<li><p><strong>禁用 vllm 相关功能</strong></p>
<p>由于 vLLM 不支持 Windows 环境，为避免引发相关错误，需禁用项目中涉及 vLLM 的所有功能模块。具体步骤如下：</p>
</li>
</ol>
<ul>
<li><p><strong>注释掉 vllm 引用</strong></p>
<p>  打开文件 <code>Weclone/weclone/data/clean/strategies.py</code>，找到以下导入语句，并将 vllm 的引用注释掉：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Weclone/weclone/data/clean/strategies.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Dict</span>, <span class="type">List</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> weclone.data.models <span class="keyword">import</span> QaPair, CutMessage, QaPairScore</span><br><span class="line"><span class="keyword">from</span> weclone.prompts.clean_data <span class="keyword">import</span> CLEAN_PROMPT</span><br><span class="line"><span class="comment"># from weclone.core.inference.vllm_infer import infer as infer  # 注释此行</span></span><br><span class="line"><span class="keyword">from</span> weclone.utils.log <span class="keyword">import</span> logger</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>禁用数据清洗功能</strong></p>
<p>打开 <code>settings.jsonc</code>，找到 <code>clean_dataset</code> 配置项，将 <code>enable_clean</code> 的值设置为 <code>false</code>，如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;clean_dataset&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enable_clean&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span>  <span class="comment">// 将 true 改为 false，禁用数据清洗</span></span><br><span class="line">    <span class="attr">&quot;clean_strategy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llm&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;llm&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;accept_score&quot;</span><span class="punctuation">:</span> <span class="number">2</span>  <span class="comment">// 可接受的 LLM 打分阈值（1分最差，5分最好），低于该分数的数据将被忽略</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>这样设置后，系统将跳过基于大模型的自动数据清洗过程。</p>
</li>
<li><p><strong>屏蔽 <code>lamafactory</code> 自动加载 vLLM</strong></p>
<p>  <code>llamafactory</code> 在导入过程中可能尝试加载 <code>vllm_engine</code>，在 Windows 下会导致报错。为防止此类问题，请在以下两个文件最顶部插入 mock 脚本：</p>
<ul>
<li><code>WeClone/weclone/eval/web_demo.py</code></li>
<li><code>WeClone/weclone/server/api_service.py</code></li>
</ul>
<p>  插入以下内容作为文件开头（必须在其他导入语句之前）：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> types</span><br><span class="line"><span class="keyword">from</span> unittest.mock <span class="keyword">import</span> MagicMock</span><br><span class="line"></span><br><span class="line">fake_vllm_engine = types.ModuleType(<span class="string">&#x27;vllm_engine&#x27;</span>)</span><br><span class="line">fake_vllm_engine.VllmEngine = MagicMock</span><br><span class="line">sys.modules[<span class="string">&#x27;llamafactory.chat.vllm_engine&#x27;</span>] = fake_vllm_engine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为原始文件的其他代码</span></span><br></pre></td></tr></table></figure>

  </details>
  
<hr>
</li>
</ul>
<p><strong>到这里，恭喜你完成了全部的环境配置。你已经完成了整个项目部署最难的部分！！！</strong></p>
<h2 id="使用-PyWxDump-提取微信聊天记录"><a href="#使用-PyWxDump-提取微信聊天记录" class="headerlink" title="使用 PyWxDump 提取微信聊天记录"></a>使用 PyWxDump 提取微信聊天记录</h2><p>要微调模型，首先你需要你的微信聊天数据。</p>
<h3 id="下载并安装PyWxDump："><a href="#下载并安装PyWxDump：" class="headerlink" title="下载并安装PyWxDump："></a><strong>下载并安装PyWxDump：</strong></h3><p><code>PyWxDump</code>是一个用于提取微信聊天记录的工具。由于<code>PyWxDump</code>目前仅确认在Windows环境下正常运行，所有无论使用<code>WSL2</code>还是纯Windows环境部署的<code>WeClone</code>，这里都切换到<strong>Windows环境</strong>。</p>
<p><strong>在Windows环境下</strong>访问 <a target="_blank" rel="noopener" href="https://github.com/xaoyaoo/PyWxDump">PyWxDump GitHub 仓库</a> 获取最新版本安装包。安装教程可直接参考<a target="_blank" rel="noopener" href="https://github.com/xaoyaoo/PyWxDump/blob/master/doc/UserGuide.md">PyWxDump官方教程</a></p>
<h3 id="导出数据："><a href="#导出数据：" class="headerlink" title="导出数据："></a><strong>导出数据：</strong></h3><ul>
<li>根据 PyWxDump 的指南，运行软件并解密你的微信数据库</li>
<li>在 PyWxDump 中选择“聊天备份”功能</li>
<li>导出类型选择 CSV</li>
<li>你可以选择导出与多个联系人或群聊的聊天记录（当前版本不建议使用群聊记录）</li>
</ul>
<h3 id="整理数据："><a href="#整理数据：" class="headerlink" title="整理数据："></a><strong>整理数据：</strong></h3><ul>
<li><p>PyWxDump 导出的 CSV 文件通常位于其运行目录下的 <code>wxdump_tmp/export</code> 文件夹中。</p>
</li>
<li><p>将整个 <code>csv</code> 文件夹 (其中可能包含多个代表不同聊天对象的子文件夹，每个子文件夹里是对应的聊天记录CSV文件) <strong>移动或复制</strong>到 WeClone 项目的 <code>./dataset/</code> 目录下。</p>
</li>
<li><p>因为<code>WSL2</code>和<code>Windows</code>环境实际上是互通的，对于使用<code>WSL2</code>部署的用户可使用以下命令将<code>Windows</code>环境下的文件复制到<code>WSL2</code>的项目目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> -r /mnt/你的PyWxDump/csv ./dataset/ <span class="comment">#在WeClone根目录下执行该命令</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#例如：</span></span><br><span class="line"><span class="built_in">cp</span> -r /mnt/d/Desktop/Just_for_fun/wxdump_work/export/wxid_wk5iejbp9ma322/csv ./dataset/</span><br></pre></td></tr></table></figure>
</li>
<li><p>最终的目录结构应类似于：<code>WeClone/dataset/csv/张三/聊天记录.csv</code> 等。</p>
</li>
</ul>
<details><summary><strong>PyWxDump操作流程图解</strong></summary><br>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B0.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B1.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B3.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B4.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B002.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B001.png" width="90%"/>
</p>
</details>
---


<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>原始的聊天记录需要经过预处理才能用于模型训练。</p>
<ul>
<li><strong>默认处理：</strong> WeClone 项目默认会去除数据中的手机号、身份证号、邮箱和网址。</li>
<li><strong>自定义过滤（可选）：</strong> 项目提供了一个禁用词词库 ，在最新的代码中这个词库被移动到了<code>settings.jsonc</code>的<code>blocked_words</code>。你可以向其中添加不希望出现在训练数据中的词句（包含禁用词的整句会被过滤掉）。</li>
</ul>
<h3 id="执行预处理脚本："><a href="#执行预处理脚本：" class="headerlink" title="执行预处理脚本："></a><strong>执行预处理脚本：</strong></h3><p>在激活虚拟环境的命令行中，进入 WeClone 项目根目录，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli make-dataset  <span class="comment">#在WeClone根目录下执行该命令</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>WeClone</code>默认启用了<code>clean_dataset</code>中的<code>enable_clean</code>选项，对数据进行清洗，以达到更好效果。当前使用<code>llm judge</code>对聊天记录进行打分，使用vllm进行离线推理。在得到<code>llm打分分数分布情况</code>后，调整<code>accept_score</code>选择可以接受的分数，再适当降低<code>train_sft_args</code>的<code>lora_dropout</code>参数提升拟合效果。纯<code>Windows</code>平台部署的用户无法使用该功能，已在环境配置（纯 Windows 版）的第<code>9</code>小节说明。</p>
</li>
<li><p>如果你启用了<code>clean_dataset</code>中的<code>enable_clean</code>，且显存有限，需要启用**<code>vllm</code>的 <code>bitsandbytes</code> 的量化加载**，否则这一步也可能会爆显存。进一步调整、优化<code>vllm</code>参数请查询<a target="_blank" rel="noopener" href="https://docs.vllm.com.cn/en/latest/serving/engine_args.html#engine-args"> vLLM 引擎参数 </a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在下面代码中的engine_args新增参数</span></span><br><span class="line"><span class="comment"># weclone/core/inference/vllm_infer.py</span></span><br><span class="line"></span><br><span class="line">engine_args = &#123;</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model_args.model_name_or_path,</span><br><span class="line">    <span class="string">&quot;trust_remote_code&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&quot;dtype&quot;</span>: model_args.infer_dtype</span><br><span class="line">    <span class="string">&quot;max_model_len&quot;</span>: cutoff_len + max_new_tokens,</span><br><span class="line">    <span class="string">&quot;enable_lora&quot;</span>: model_args.adapter_name_or_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">    <span class="string">&quot;enable_prefix_caching&quot;</span>: <span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ↓ 新增内容 ↓</span></span><br><span class="line">    <span class="string">&quot;quantization&quot;</span>: <span class="string">&quot;bitsandbytes&quot;</span>,</span><br><span class="line">    <span class="string">&quot;load_format&quot;</span>: <span class="string">&quot;bitsandbytes&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><p>如果遇到报错<code>ImportError: Please install bitsandbytes&gt;=0.45.3</code>，可以尝试重新安装<code>bitsandbytes</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用uv安装 bitsandbytes，建议科学上网</span></span><br><span class="line">uv pip install bitsandbytes&gt;=0.39.0</span><br></pre></td></tr></table></figure></li>
</ul>
</blockquote>
</li>
<li><p>预处理后的数据通常会保存在 <code>\WeClone\dataset\res_csv\sft</code> 或类似目录下的<code>sft-my.json</code>文件中。</p>
</li>
</ul>
<h2 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h2><p>WeClone 默认使用 Qwen2.5-7B-Instruct 模型，你也可以选择其他你想要使用的模型（不推荐使用深度思考模型）。</p>
<h3 id="安装-Git-LFS："><a href="#安装-Git-LFS：" class="headerlink" title="安装 Git LFS："></a><strong>安装 Git LFS：</strong></h3><p>在 CMD 或 PowerShell 窗口，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br></pre></td></tr></table></figure>
<p>每个用户只需执行一次此命令。</p>
<h3 id="克隆模型仓库："><a href="#克隆模型仓库：" class="headerlink" title="克隆模型仓库："></a><strong>克隆模型仓库：</strong></h3><p>推荐使用 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/models">魔搭社区（ModelScope）</a> 的模型资源，默认下载 <code>Qwen2.5-7B-Instruct</code> 模型。你也可以根据自己的情况和喜好选择该平台的其他模型。</p>
<p>在项目根目录下执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git</span><br></pre></td></tr></table></figure>
<blockquote>
<p>⚠️ 模型文件较大，下载时间可能较长，请确保磁盘空间充足（建议至少 20GB）。</p>
</blockquote>
<p>如果你选择了其他模型，或将模型下载到了不同目录，请在 <code>settings.jsonc</code> 中修改路径：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;common_args&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你的模型路径&quot;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;template&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你的模型模板&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>




<h2 id="修改配置文件-settings-jsonc"><a href="#修改配置文件-settings-jsonc" class="headerlink" title="修改配置文件 (settings.jsonc)"></a>修改配置文件 (<code>settings.jsonc</code>)</h2><p>WeClone 项目的训练与推理核心配置集中在 <code>settings.jsonc</code> 文件中。你需要根据实际使用场景，<strong>适当调整其中的关键参数</strong>。</p>
<p>请打开项目根目录下的 <code>WeClone/settings.jsonc</code> 文件，重点关注以下配置项：</p>
<p><strong>训练相关参数：</strong></p>
<ul>
<li><p><strong><code>per_device_train_batch_size</code></strong> 和 <strong><code>gradient_accumulation_steps</code></strong>：<br> 控制单张显卡的显存占用与有效 batch size。可根据显存大小调整，达到资源利用与训练稳定性的平衡。</p>
<blockquote>
<p><strong>调节建议：</strong></p>
<ul>
<li><strong>显存较小</strong>：减小 <code>per_device_train_batch_size</code>，增大 <code>gradient_accumulation_steps</code>；</li>
<li>显存充足：可适当增大 <code>per_device_train_batch_size</code>，以加快训练速度；</li>
</ul>
</blockquote>
</li>
<li><p><strong><code>train_sft_args</code> 中的参数</strong>（适用于 SFT 阶段微调）：<br>可根据数据量与任务复杂度调整以下参数：</p>
<ul>
<li><code>num_train_epochs</code>：训练轮数；</li>
<li><code>lora_rank</code>：LoRA 子空间维度，越高越耗显存；</li>
<li><code>lora_dropout</code>：LoRA dropout 比例，用于防止过拟合。</li>
</ul>
</li>
<li><p><strong>建议：</strong></p>
<ul>
<li>配置文件中包含详细注释，建议逐条阅读理解后再做修改。</li>
<li>若你希望使用其他微调策略（如全量微调、Freeze 等），请确保 <code>finetuning_type</code> 与相关参数保持一致。</li>
<li>进一步了解参数，请查看<code>LLaMA Factory</code>官方文档：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html">参数介绍 - LLaMA Factory</a></li>
</ul>
</li>
</ul>
<h3 id="启用-QLoRA（可选配置）"><a href="#启用-QLoRA（可选配置）" class="headerlink" title="启用 QLoRA（可选配置）"></a><strong>启用 QLoRA（可选配置）</strong></h3><p>如果你希望进一步减少显存消耗（尤其在显存紧张场景下），可以开启 <strong>QLoRA 量化训练</strong>。</p>
<p>在 <code>settings.jsonc</code> 的 <code>common_args</code> 字段中添加以下配置：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;quantization_bit&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;quantization_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nf4&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;double_quantization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;quantization_method&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bitsandbytes&quot;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意：</strong></p>
<ul>
<li><p><code>quantization_bit</code> 支持值：2 &#x2F; 4 &#x2F; 8，数值越低显存越省，但推理速度和效果可能略有下降。</p>
</li>
<li><p>如果遇到报错<code>ImportError: Please install bitsandbytes&gt;=0.45.3</code>，可以尝试重新安装<code>bitsandbytes</code>：</p>
</li>
</ul>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用uv安装 bitsandbytes，建议科学上网</span></span><br><span class="line">uv pip install bitsandbytes&gt;=0.39.0</span><br></pre></td></tr></table></figure></blockquote>
<p><strong>调整建议：</strong></p>
<ul>
<li>配置文件中包含详细注释，建议逐条阅读理解后再做修改。</li>
<li>若你希望使用其他微调策略（如全量微调、Freeze、QLoRA 等），请确保 <code>finetuning_type</code> 与相关参数保持一致。</li>
<li>进一步了解参数详情请查看<code>LLaMA Factory</code>官方文档：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html">参数介绍 - LLaMA Factory</a></li>
</ul>
<details>
<summary>Linux编辑文件操作教程（`nano`编辑器简要教程）</summary>

<p>如果你是用的是WSL2部署，且不熟悉Linux的基本操作，可以按照下面操作：</p>
<ol>
<li>在根目录下运行<code>nano</code>编辑器</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nano settings.jsonc </span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>移动光标到你要修改的位置，进行对应修改操作。</p>
<p><code>Ctrl + ^</code> 是标记开始（英文叫 “set mark”）；</p>
<p><code>Alt + 6</code> 是复制（保留内容），<code>Ctrl + K</code> 是剪切（删除原内容）；<code>Ctrl + U</code> 是粘贴(仅粘贴内部复制或剪切内容)；</p>
<p><code>鼠标右键</code>是粘贴外部内容。</p>
</li>
<li><p>修改后按 <code>Ctrl + O</code> 保存，按 <code>Enter</code> 确认；再按 <code>Ctrl + X</code> 退出。</p>
</li>
</ol>
</details>


<hr>
<p><strong>到这里你已经完成所有前期的配置工作了，接下来即将开始正式推理、训练模型！！！</strong></p>
<h2 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h2><p>配置完成后，就可以开始训练了。</p>
<h3 id="单卡训练"><a href="#单卡训练" class="headerlink" title="单卡训练"></a>单卡训练</h3><p>在激活虚拟环境的命令行中，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli train-sft</span><br></pre></td></tr></table></figure>

<blockquote>
<p>多卡环境单卡训练，需要先执行 <code>export CUDA_VISIBLE_DEVICES=0</code></p>
</blockquote>
<p>训练脚本会读取 <code>settings.jsonc</code> 中的配置并开始微调。留意终端输出，观察 loss 是否在正常下降。</p>
<h3 id="多卡训练"><a href="#多卡训练" class="headerlink" title="多卡训练"></a>多卡训练</h3><p>如果你有多张 NVIDIA GPU 并希望进行多卡训练：</p>
<ol>
<li><p><strong>安装 Deepspeed：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install deepspeed</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[!WARNING]<br>Deepspeed 在 Windows 上的原生支持可能有限或配置复杂。官方主要支持 Linux。如果遇到安装或运行问题，可能需要查阅 Deepspeed 官方文档或社区寻求 Windows 解决方案，或者考虑在 WSL2 环境下使用 Deepspeed。</p>
</blockquote>
</li>
<li><p><strong>配置 Deepspeed：</strong><br>在 <code>settings.jsonc</code> 中，找到 <code>deepspeed</code> 配置项，并取消其注释或根据需要填写 Deepspeed 的 JSON 配置文件路径。</p>
</li>
<li><p><strong>启动多卡训练：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --num_gpus=&lt;使用显卡数量&gt; weclone/train/train_sft.py</span><br></pre></td></tr></table></figure>

<p>例如，使用2张显卡：<code>deepspeed --num_gpus=2 weclone/train/train_sft.py</code></p>
</li>
</ol>
<p>训练完成后，微调好的 LoRA 适配器权重会保存在你 <code>settings.jsonc</code> 中指定的 <code>output_dir</code>。</p>
<h2 id="推理-与你的数字分身对话"><a href="#推理-与你的数字分身对话" class="headerlink" title="推理 (与你的数字分身对话)"></a>推理 (与你的数字分身对话)</h2><p>微调完成后，你可以通过以下几种方式与你的数字分身进行交互。</p>
<h3 id="使用浏览器-Demo-简单推理"><a href="#使用浏览器-Demo-简单推理" class="headerlink" title="使用浏览器 Demo 简单推理"></a>使用浏览器 Demo 简单推理</h3><p>这是一种快速测试模型效果并调整推理参数（如 <code>temperature</code>, <code>top_p</code>）的方法。<br>在激活虚拟环境的命令行中，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli webchat-demo</span><br></pre></td></tr></table></figure>

<p>脚本会启动一个本地 Web 服务 (通常在 <code>http://127.0.0.1:7860</code> 或类似地址)，你可以在浏览器中打开它进行对话。在这里测试出的最佳推理参数可以更新回 <code>settings.jsonc</code> 的 <code>infer_args</code> 部分，供后续使用。</p>
<h3 id="使用-API-接口进行推理"><a href="#使用-API-接口进行推理" class="headerlink" title="使用 API 接口进行推理"></a>使用 API 接口进行推理</h3><p>WeClone 提供了一个 API 服务，可以供其他应用程序调用。</p>
<ol>
<li><p><strong>启动 API 服务：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli server</span><br></pre></td></tr></table></figure>

<p>服务启动后，通常会监听在 <code>http://127.0.0.1:8005/v1</code> (具体地址和端口请查看终端输出或 <code>settings.jsonc</code> 中的配置)。</p>
</li>
<li><p><strong>通过 API 调用：</strong><br>你可以使用任何 HTTP客户端 (如 Postman, curl，或 Python 的 <code>requests</code> 库) 向该 API 发送请求。API 通常兼容 OpenAI 的格式。</p>
</li>
</ol>
<h3 id="使用常见聊天问题测试"><a href="#使用常见聊天问题测试" class="headerlink" title="使用常见聊天问题测试"></a>使用常见聊天问题测试</h3><p>项目还提供了一个脚本，可以使用预设的问题列表来测试模型。</p>
<ol>
<li>确保 API 服务 (<code>weclone-cli server</code>) 正在运行。</li>
<li>打开一个新的命令行窗口 (并激活虚拟环境)，然后运行：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli test-model</span><br></pre></td></tr></table></figure>
测试结果会输出到 <code>test_result-my.txt</code>。</li>
</ol>
<h2 id="通过-AstrBot-部署到聊天机器人"><a href="#通过-AstrBot-部署到聊天机器人" class="headerlink" title="通过 AstrBot 部署到聊天机器人"></a>通过 AstrBot 部署到聊天机器人</h2><p><a target="_blank" rel="noopener" href="https://github.com/AstrBotDevs/AstrBot">AstrBot</a> 是一个支持多平台的 LLM 聊天机器人框架，可以将你的 WeClone 模型部署到 QQ、微信、Telegram 等平台。</p>
<p>以下是大致的部署步骤：</p>
<ol>
<li><p><strong>部署 AstrBot：</strong> 根据 AstrBot 官方文档的指引，在你的服务器或本地安装并配置好 AstrBot。</p>
<p><a target="_blank" rel="noopener" href="https://astrbot.app/deploy/astrbot/windows.html">使用 Windows 一键安装器部署 AstrBot | AstrBot</a></p>
</li>
<li><p><strong>启动 WeClone API 服务：</strong> 确保你的 <code>weclone-cli server</code> 正在运行，并且 AstrBot 可以访问到该服务的地址和端口 (例如，如果 AstrBot 和 API 服务在同一台机器上，可能是 <code>http://127.0.0.1:8005/v1</code>）。</p>
</li>
<li><p><strong>在 AstrBot 中新增服务提供商：</strong></p>
<ul>
<li>类型选择：OpenAI</li>
<li>API Base URL：填写你的 WeClone API 服务地址 (如果部署在本地则填写 <code>http://127.0.0.1:8005/v1</code>)。</li>
<li>模型：可以填写一个占位符，如 <code>gpt-3.5-turbo</code> (因为实际使用的模型由 WeClone API 服务决定)。</li>
<li>API Key：随意填写一个即可。</li>
</ul>
</li>
<li><p><strong>在 AstrBot 中部署消息平台：</strong> 配置 AstrBot 以连接到你希望使用的聊天平台 (如微信、QQ 等)。如果你想要部署到QQ，可以参考下面AstrBot提供的教程：<a target="_blank" rel="noopener" href="https://astrbot.app/deploy/platform/aiocqhttp/napcat.html#%E9%80%9A%E8%BF%87-napcatqq-%E5%8D%8F%E8%AE%AE%E5%AE%9E%E7%8E%B0%E7%AB%AF%E6%8E%A5%E5%85%A5-qq">通过 NapCatQQ 协议实现端接入 QQ | AstrBot</a></p>
</li>
<li><p><strong>关闭工具调用 (重要)：</strong><br>微调后的模型主要用于模仿你的语言风格，通常不支持复杂的工具调用。在 AstrBot 对应的聊天平台中，向你的机器人发送指令关闭所有默认工具，以确保能看到微调效果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/tool off all</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>设置系统提示词：</strong><br>在 AstrBot 的配置中，为你的机器人设置<strong>系统提示词 (System Prompt)</strong>。这个提示词必须与你微调模型时在 <code>settings.jsonc</code> 中设置的 <code>default_system</code> <strong>完全一致</strong>。</p>
<img src="https://blog-img.051088.xyz/AstrBot%E6%95%99%E7%A8%8B01.png"/>
</li>
<li><p><strong>调整采样参数：</strong><br>根据你在浏览器 Demo 或其他测试中得到的最佳效果，在 AstrBot 中调整模型的采样参数，如 <code>temperature</code>, <code>top_p</code>, <code>top_k</code> 等。具体配置方法请参考 AstrBot 文档中关于<a target="_blank" rel="noopener" href="https://www.google.com/search?q=https://astrbot.app/config/model-config.html%23%25E9%2585%258D%25E7%25BD%25AE%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589%25E7%259A%2584%25E6%25A8%25A1%25E5%259E%258B%25E5%258F%2582%25E6%2595%25B0">配置自定义模型参数</a>的部分。</p>
</li>
</ol>
<blockquote>
<p>[!IMPORTANT]<br>经常检查 <code>api_service.py</code> 的日志输出，确保 AstrBot 发送给大模型服务的请求参数 (如 system prompt, temperature 等) 与你微调和测试时期望的一致。</p>
</blockquote>
<p>现在，你的专属数字分身应该已经成功部署到聊天机器人平台了！快去和“它”聊聊天，看看效果如何吧。</p>
<h2 id="问题解决与支持"><a href="#问题解决与支持" class="headerlink" title="问题解决与支持"></a>问题解决与支持</h2><ul>
<li><strong>微调问题：</strong> 可以参考 LLaMA Factory 的 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory/issues/4614">FAQs | 常见问题</a>，因为 WeClone 底层使用了 LLaMA Factory 的部分组件或类似逻辑。</li>
<li><strong>项目 Issues：</strong> 查看 WeClone GitHub 仓库的 <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone/issues">Issues</a> 和 <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone/discussions">Discussions</a> 是否有类似问题和解决方案。</li>
</ul>
<h2 id="免责声明"><a href="#免责声明" class="headerlink" title="免责声明"></a>免责声明</h2><blockquote>
<p>[!CAUTION]<br>请勿用于非法用途，否则后果自负。<br>本教程仅供学习交流使用。任何违反法律法规、侵犯他人合法权益的行为，均与WeClone项目及笔者无关。严禁用于窃取他人隐私。</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone?tab=readme-ov-file">xming521&#x2F;WeClone – 从聊天记录创造数字分身的一站式解决方案</a>：项目主页，涵盖聊天记录处理、LoRA 微调、声音克隆等模块，本文主要基于项目README编写。</li>
</ul>
<blockquote>
<p>其余引用链接已在正文中明确标注。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.051088.xyz/">MarkBai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/">https://blog.051088.xyz/2025/05/14/WeClone-用微信聊天记录打造自己的AI数字分身/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.051088.xyz" target="_blank">MarkBai的数字自留地</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/WeClone/">WeClone</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/">数字分身</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大语言模型</a><a class="post-meta__tags" href="/tags/%E6%8C%87%E5%8D%97/">指南</a></div><div class="post-share"><div class="social-share" data-image="https://blog-img.051088.xyz/weclone.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客"><img class="cover" src="https://blog-img.051088.xyz/2009-2024-08-23043351-1724402031271.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">我的第一篇博客</div></div><div class="info-2"><div class="info-item-1">欢迎光临我的数字自留地    温馨提示：本博客仍处于“装修进行时”状态，页面略显朴素，功能还有待打磨，top-img 甚至是我随手挑的一张壁纸。但我实在是太想发点啥了，所以就这样吧！   🧑‍💻 Who am I？你好呀！我是来自华南理工大学土木工程专业的大四老油条，目前正处于毕业设计焦头烂额的阶段。不过说实话，我真正沉迷的不是混凝土和钢筋，而是代码与芯片的世界。 我课余时间辅修了计算机相关课程，虽然不敢说学得多深入，但在一次次踩坑和尝试中，我确实越来越喜欢折腾技术。喜欢把脑子里那些天马行空的想法，通过计算机一步步“落地成型”。这种“从 0 到 1”的成就感，是我一直想要保留的东西。  🤖 关于 AI 那些事从我21年进入大学开始，AI 就像坐上了火箭一样往前冲。ChatGPT、DeepSeek、Stable Diffusion、ComfyUI、ASR、TTS、AI Agent、MCP……每年甚至每一天都会有新词汇刷屏，我也一直在“浅踩坑+深入玩”的路上。 刚开始是单纯地觉得 Magic，后来慢慢地想了解它是怎么运作的，再后来，甚至开始琢磨能不能自己搞点东西。从调教大模型...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/profile-photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">MarkBai</div><div class="author-info-description">技术博客，记录计算机、AI、单片机与结构工程领域的折腾与探索
</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/BAIKEMARK"><i class="fab fa-github"></i><span>My Github</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/BAIKEMARK" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:blog@051088.xyz" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://space.bilibili.com/490783536?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="fa-brands fa-bilibili" style="color: #00a5dc;"></i></a><a class="social-icon" href="https://www.xiaohongshu.com/user/profile/607994f60000000001007812" target="_blank" title="小红书"><i class="fab fa-xiaohongshu" style="color: #ff3b30;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">WeClone教程更新啦！！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#WeClone%EF%BC%9A%E8%AE%A9%E4%BD%A0%E7%9A%84%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%8B%A5%E6%9C%89%E2%80%9C%E7%81%B5%E9%AD%82%E2%80%9D"><span class="toc-number">1.</span> <span class="toc-text">WeClone：让你的聊天记录拥有“灵魂”</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%80%E9%9C%80%E7%A1%AC%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text">所需硬件配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">3.</span> <span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%88-WSL2%E7%89%88%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">环境配置（ WSL2版）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85WSL2"><span class="toc-number">3.1.1.</span> <span class="toc-text">安装WSL2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Git-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6"><span class="toc-number">3.1.2.</span> <span class="toc-text">Git 版本控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%B9%B6%E5%AE%89%E8%A3%85-WeClone-%E4%BE%9D%E8%B5%96"><span class="toc-number">3.1.3.</span> <span class="toc-text">创建虚拟环境并安装 WeClone 依赖</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%88%E7%BA%AF-Windows-%E7%89%88%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">环境配置（纯 Windows 版）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Git-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">Git 版本控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%B9%B6%E5%AE%89%E8%A3%85-WeClone-%E4%BE%9D%E8%B5%96-1"><span class="toc-number">3.2.2.</span> <span class="toc-text">创建虚拟环境并安装 WeClone 依赖</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-PyWxDump-%E6%8F%90%E5%8F%96%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95"><span class="toc-number">4.</span> <span class="toc-text">使用 PyWxDump 提取微信聊天记录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85PyWxDump%EF%BC%9A"><span class="toc-number">4.1.</span> <span class="toc-text">下载并安装PyWxDump：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-number">4.2.</span> <span class="toc-text">导出数据：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E7%90%86%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-number">4.3.</span> <span class="toc-text">整理数据：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E9%A2%84%E5%A4%84%E7%90%86%E8%84%9A%E6%9C%AC%EF%BC%9A"><span class="toc-number">5.1.</span> <span class="toc-text">执行预处理脚本：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"><span class="toc-number">6.</span> <span class="toc-text">模型下载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Git-LFS%EF%BC%9A"><span class="toc-number">6.1.</span> <span class="toc-text">安装 Git LFS：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%8B%E9%9A%86%E6%A8%A1%E5%9E%8B%E4%BB%93%E5%BA%93%EF%BC%9A"><span class="toc-number">6.2.</span> <span class="toc-text">克隆模型仓库：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-settings-jsonc"><span class="toc-number">7.</span> <span class="toc-text">修改配置文件 (settings.jsonc)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E7%94%A8-QLoRA%EF%BC%88%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">启用 QLoRA（可选配置）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.</span> <span class="toc-text">微调模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="toc-number">8.1.</span> <span class="toc-text">单卡训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="toc-number">8.2.</span> <span class="toc-text">多卡训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E7%90%86-%E4%B8%8E%E4%BD%A0%E7%9A%84%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB%E5%AF%B9%E8%AF%9D"><span class="toc-number">9.</span> <span class="toc-text">推理 (与你的数字分身对话)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%B5%8F%E8%A7%88%E5%99%A8-Demo-%E7%AE%80%E5%8D%95%E6%8E%A8%E7%90%86"><span class="toc-number">9.1.</span> <span class="toc-text">使用浏览器 Demo 简单推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-API-%E6%8E%A5%E5%8F%A3%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="toc-number">9.2.</span> <span class="toc-text">使用 API 接口进行推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B8%B8%E8%A7%81%E8%81%8A%E5%A4%A9%E9%97%AE%E9%A2%98%E6%B5%8B%E8%AF%95"><span class="toc-number">9.3.</span> <span class="toc-text">使用常见聊天问题测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87-AstrBot-%E9%83%A8%E7%BD%B2%E5%88%B0%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA"><span class="toc-number">10.</span> <span class="toc-text">通过 AstrBot 部署到聊天机器人</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E4%B8%8E%E6%94%AF%E6%8C%81"><span class="toc-number">11.</span> <span class="toc-text">问题解决与支持</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E"><span class="toc-number">12.</span> <span class="toc-text">免责声明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">13.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身"><img src="https://blog-img.051088.xyz/weclone.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WeClone — 用微信聊天记录打造自己的 AI 数字分身"/></a><div class="content"><a class="title" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身">WeClone — 用微信聊天记录打造自己的 AI 数字分身</a><time datetime="2025-05-14T14:00:00.000Z" title="发表于 2025-05-14 22:00:00">2025-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客"><img src="https://blog-img.051088.xyz/2009-2024-08-23043351-1724402031271.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="我的第一篇博客"/></a><div class="content"><a class="title" href="/2025/04/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客">我的第一篇博客</a><time datetime="2025-04-19T12:30:00.000Z" title="发表于 2025-04-19 20:30:00">2025-04-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://blog-img.051088.xyz/wallhaven-5gwvz5_1920x1080.png);"><div id="footer-wrap"><div class="copyright">&copy;2025 By MarkBai</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">Hi, welcome to <a href="https://blog.051088.xyz/">MarkBai's Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.051088.xyz',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.051088.xyz',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script id="canvas_nest" defer="defer" color="100,200,255" opacity="0.7" zIndex="-1" count="66" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="GOOD,LUCK,FOR,YOU" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>