<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>WeClone：基于 Linux 的部署指南【保姆级教程】 | MarkBai的数字自留地</title><meta name="author" content="MarkBai,blog@051088.xyz"><meta name="copyright" content="MarkBai"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本指南演示如何在 Linux 环境下使用 xming521&#x2F;WeClone 项目，创造一个拥有你“那味儿”的数字分身。">
<meta property="og:type" content="article">
<meta property="og:title" content="WeClone：基于 Linux 的部署指南【保姆级教程】">
<meta property="og:url" content="https://blog.051088.xyz/posts/weclone-linux-tutorial/index.html">
<meta property="og:site_name" content="MarkBai的数字自留地">
<meta property="og:description" content="本指南演示如何在 Linux 环境下使用 xming521&#x2F;WeClone 项目，创造一个拥有你“那味儿”的数字分身。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-img.051088.xyz/weclone.png">
<meta property="article:published_time" content="2025-05-22T09:00:00.000Z">
<meta property="article:modified_time" content="2025-05-28T07:50:19.547Z">
<meta property="article:author" content="MarkBai">
<meta property="article:tag" content="WeClone">
<meta property="article:tag" content="数字分身">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="大语言模型">
<meta property="article:tag" content="指南">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-img.051088.xyz/weclone.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "WeClone：基于 Linux 的部署指南【保姆级教程】",
  "url": "https://blog.051088.xyz/posts/weclone-linux-tutorial/",
  "image": "https://blog-img.051088.xyz/weclone.png",
  "datePublished": "2025-05-22T09:00:00.000Z",
  "dateModified": "2025-05-28T07:50:19.547Z",
  "author": [
    {
      "@type": "Person",
      "name": "MarkBai",
      "url": "https://blog.051088.xyz/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.051088.xyz/posts/weclone-linux-tutorial/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="msvalidate.01" content="3D5AD97894B95C6A0545D582BD3EB688"/><meta name="baidu-site-verification" content="codeva-oo2hqM6DSL"/><meta name="google-site-verification" content="3pSewSvoaikv4-iDDPLHEufO0hB5ACox1m-6eB1BYSI"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?daf1aae7138e285b091e5da481b059e5";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'WeClone：基于 Linux 的部署指南【保姆级教程】',
  isHighlightShrink: undefined,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/flink-avatar.css?v=20250524"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #f9fcff;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/profile-photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(https://blog-img.051088.xyz/wallhaven-5gwvz5_1920x1080.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">MarkBai的数字自留地</span></a><a class="nav-page-title" href="/"><span class="site-name">WeClone：基于 Linux 的部署指南【保姆级教程】</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">WeClone：基于 Linux 的部署指南【保姆级教程】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-22T09:00:00.000Z" title="发表于 2025-05-22 17:00:00">2025-05-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-28T07:50:19.547Z" title="更新于 2025-05-28 15:50:19">2025-05-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/">技术教程</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/AI%E5%BA%94%E7%94%A8/">AI应用</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">8.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="WeClone：让你的聊天记录拥有“灵魂”"><a href="#WeClone：让你的聊天记录拥有“灵魂”" class="headerlink" title="WeClone：让你的聊天记录拥有“灵魂”"></a>WeClone：让你的聊天记录拥有“灵魂”</h2><p>你是否曾设想过——如果你的聊天风格、口头禅，甚至独特的表达习惯，能够被 AI 学会，并在数字世界中“复刻”一个你，会是怎样的体验？ <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone">xming521&#x2F;WeClone 项目</a> 正是在这样一个愿景下诞生的尝试。</p>
<p>WeClone 是一个一站式解决方案，它通过分析你的微信聊天记录，微调大语言模型（LLM），让模型说出“你那味儿”的话，并将其接入聊天机器人，打造出专属于你的数字分身。</p>
<details>
  <summary style="cursor: pointer; font-size: 16px; font-weight: bold; margin-bottom: 10px;">
    点击展开查看训练效果图
  </summary>
  <div style="
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: flex-start;
    gap: 20px;
    padding-top: 15px;
    width: 100%;
    box-sizing: border-box;
  ">
    <div style="flex: 1 1 400px; max-width: 800px;">
      <img 
        src="https://blog-img.051088.xyz/%E6%9C%80%E7%BB%88%E6%95%88%E6%9E%9C1.png" 
        alt="训练效果图1"
        style="
          width: 100%;
          height: auto;
          object-fit: contain;
          border-radius: 10px;
          box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        " 
        loading="lazy"
      />
    </div>
    <div style="flex: 1 1 300px; max-width: 500px;">
      <img 
        src="https://blog-img.051088.xyz/%E6%9C%80%E7%BB%88%E6%95%88%E6%9E%9C.png" 
        alt="训练效果图2"
        style="
          width: 100%;
          height: auto;
          object-fit: contain;
          border-radius: 10px;
          box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        " 
        loading="lazy"
      />
    </div>
  </div>
</details>


<hr>
<h2 id="简介（废话版）"><a href="#简介（废话版）" class="headerlink" title="简介（废话版）"></a>简介（废话版）</h2><p>WeClone 项目官方推荐使用 <strong>Linux 环境</strong> 进行部署。在实际操作中我们也发现，相较于 Windows 或 macOS，Linux 系统对大多数深度学习库支持更好，资源调度更高效，更适合模型训练等计算密集型任务。</p>
<p>同时，由于 Windows 或 macOS 用户本地设备的性能——尤其是显存容量——往往难以满足大模型微调的需求，因此租用具备更高算力的 Linux 云服务器，也成为一种常见且现实的选择。</p>
<p>不过，对于习惯图形界面操作的初学者来说，Linux 尤其是无图形界面的云服务器，可能会带来一定的上手门槛。此外，WeClone 所采用的 <strong>QLoRA 微调流程</strong> 及其他关键细节虽然在官方文档中有所提及，但讲解相对简略，对缺乏深度学习经验的用户来说，依然存在不少理解与操作上的困难。</p>
<p>因此，本文将作为项目 <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone/?tab=readme-ov-file">README</a> 的<strong>补充指南</strong>，专为新手用户撰写，手把手带你完成从云服务器租用、环境搭建，到训练出属于你自己的 AI 克隆体的完整流程。</p>
<blockquote>
<p>⚠️ <strong>重要提示：</strong><br>WeClone 项目仍在持续迭代中，目前版本为 <code>v0.2.2</code>。微调效果受多种因素影响，如模型规模、数据数量与质量等。一般而言：<strong>模型越大、数据越多、语言风格越统一，生成的数字分身就越接近原始人格特征。</strong><br>本文旨在提供一个可操作的实践参考，实际部署过程中可能仍会遇到个别技术问题，敬请理解与探索。</p>
</blockquote>
<blockquote>
<p>💡 <strong>温馨提示：</strong></p>
<p>关于租用显卡训练的成本问题：若使用项目默认的 <strong>7B 模型</strong>，并按文档教程步骤操作，整个训练过程的总花费<strong>通常不超过 5 元</strong>。</p>
</blockquote>
<blockquote>
<p>⚠️⚠️⚠️ <strong>免责声明：</strong></p>
<p>本教程以 AutoDL 算力云平台为例进行演示。但需特别说明：<strong>笔者及 WeClone 项目均与 AutoDL 平台无任何商务合作关系</strong>，所涉平台与工具仅为实践所选，不代表推荐或背书。</p>
</blockquote>
<hr>
<p>在正式开始前，请务必注意：</p>
<blockquote>
<p>⚠️ <strong>隐私第一！</strong></p>
<p>请务必妥善保护自己的聊天记录及个人信息，<strong>切勿上传至公共平台</strong>，并确保本项目不被用于任何非法用途。使用者应对自己的数据使用和行为承担全部责任。</p>
</blockquote>
<hr>
<h2 id="租借云服务器"><a href="#租借云服务器" class="headerlink" title="租借云服务器"></a>租借云服务器</h2><p><a target="_blank" rel="noopener" href="https://www.autodl.com/home">AutoDL算力云 </a>是一个提供GPU租用服务的平台，主要面向深度学习和人工智能领域的研究者和开发者。用户可以在AutoDL上租用GPU服务器，进行深度学习模型的训练和实验。平台提供了多种GPU资源，用户可以根据自己的需求和预算选择合适的GPU型号和配置。</p>
<p>首先登陆<a target="_blank" rel="noopener" href="https://www.autodl.com/home">AutoDL算力云 </a>，注册并完成实名认证（如果是学生可以完成学生认证，在校期间一直享95折优惠）。在“充值”界面点击“其他金额”根据，充值自定义金额（不充值没法租显卡）。充值后点击“算力市场”挑选显卡。</p>
<img src="https://blog-img.051088.xyz/linux003.png" style="zoom: 67%;" />



<h2 id="服务器配置选择"><a href="#服务器配置选择" class="headerlink" title="服务器配置选择"></a>服务器配置选择</h2><p>运行 WeClone，尤其是在执行模型微调阶段时，对硬件配置要求较高，其中<strong>显存（VRAM）是关键因素</strong>。你需要根据所选模型的大小，合理选择具备足够显存的 GPU 服务器。</p>
<p>项目默认使用 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct"><code>Qwen2.5-7B-Instruct</code></a> 模型，并采用 <strong>LoRA</strong> 方法进行参数高效微调，对显存的最低需求约为 <strong>16GB</strong>。</p>
<p>下表整理了不同模型规模和微调方法下的显存需求（数据参考自 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA Factory</a>）：</p>
<table>
<thead>
<tr>
<th>微调方法</th>
<th>精度 (bits)</th>
<th>7B 模型</th>
<th>14B 模型</th>
<th>30B 模型</th>
<th>70B 模型</th>
<th><code>x</code>B 模型</th>
</tr>
</thead>
<tbody><tr>
<td>Full (<code>bf16</code> &#x2F; <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code> GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code> GB</td>
</tr>
<tr>
<td>Freeze &#x2F; LoRA &#x2F; GaLore &#x2F; APOLLO</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code> GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code> GB</td>
</tr>
</tbody></table>
<p>此外，AutoDL 等云平台默认提供 <strong>30GB 系统盘</strong> 和 <strong>50GB 数据盘</strong>（我们通常将数据、模型及缓存文件放在数据盘）。对 7B 或 14B 模型来说，这一容量通常是充足的；但若你计划训练 30B 或更大规模的模型，<strong>建议提前扩容数据盘</strong>，以免中途因存储不足导致训练失败。</p>
<p>对于不同 GPU 服务器的配置与性能，可参考 AutoDL 帮助文档中的<a target="_blank" rel="noopener" href="https://www.autodl.com/docs/gpu/#gpu_2">GPU型号简介</a>，里面提供了各型号 GPU 的介绍与选择建议。7B模型推荐直接租4090，经笔者测试，其cuda版本和性能均满足项目要求，且在不使用QLora的条件下也能完成各部分训练。</p>
<p>值得注意的是，不同版本 GPU 所支持的 <strong>CUDA 版本</strong> 并不一致。本项目推荐使用 <strong>CUDA 12.4 及以上版本</strong>，部分用户反馈使用 CUDA 11.x 也能成功部署，但笔者<strong>未在此版本上亲测</strong>，建议优先选择官方推荐版本以避免不必要的兼容性问题。</p>
<blockquote>
<p>📌 <strong>提示</strong><br> 若你打算采用 <strong>QLoRA</strong> 微调方式，请继续阅读后续章节“<a href="#%E5%90%AF%E7%94%A8-QLoRA-%EF%BC%88%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%EF%BC%89">修改配置文件 (settings.jsonc)</a>”，了解如何正确配置微调策略与参数。</p>
</blockquote>
<h2 id="服务器购买与初始化"><a href="#服务器购买与初始化" class="headerlink" title="服务器购买与初始化"></a><strong>服务器购买与初始化</strong></h2><p>在明确自身需求后，你可以前往 <strong>算力市场</strong>，选择合适的 GPU 服务器，并进入「<strong>创建实例</strong>」页面。</p>
<p><img src="https://blog-img.051088.xyz/linux004.png" alt="img"></p>
<p>在该界面按如下路径选择镜像：<br> <strong>镜像 → 基础镜像 → PyTorch → 2.5.1 → Python 3.10 (Ubuntu 22.04) → CUDA 12.4</strong></p>
<p>确认无误后，点击右下角「<strong>创建并开机</strong>」，即完成服务器的购买与初始化。</p>
<p>如果出现如下提示：</p>
<blockquote>
<p>“镜像的 CUDA 版本高于主机支持的最高 CUDA 版本，将导致无法开机，请重新选择主机&#x2F;镜像。”</p>
</blockquote>
<p>则说明你所选 GPU 不支持 CUDA 12.4。此时可以选择较低版本的 PyTorch 镜像（如 <strong>2.1.2</strong>），以匹配当前算力资源的兼容性。</p>
<p><img src="https://blog-img.051088.xyz/linux005-1.png" alt="img"></p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h3><p>成功创建实例后，你将进入**「容器实例」**页面。在 AutoDL 提供的环境中，你可以通过 <strong>SSH 登录</strong> 和 <strong>在线 JupyterLab</strong> 两种方式连接并操作服务器。推荐使用 <strong>VS Code 的 Remote-SSH 插件</strong> 进行远程开发，同时结合在线 JupyterLab 实现便捷的文件上传。</p>
<p>此外，在「快捷工具」中还可找到 <strong>【AutoPanel】</strong> 入口，用于实时监测远程服务器的 GPU、CPU、内存与磁盘使用情况，便于资源管理与调试。</p>
<p><img src="https://blog-img.051088.xyz/linux006-1.png" alt="img"></p>
<blockquote>
<p>💡 <strong>新手提示</strong>：<br>如果你尚未安装 <strong>VS Code</strong>，或不熟悉如何通过它连接到<strong>Auto DL</strong>远程服务器，可以参考以下教程：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhaxun/article/details/120568402">VSCode 连接远程服务器（傻瓜式教学） - CSDN 博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.autodl.com/docs/vscode/">VS Code远程开发Auto DL（Auto DL官方文档）</a></p>
</blockquote>
<p>连接成功后，建议首先切换到数据盘目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/autodl-tmp</span><br></pre></td></tr></table></figure>

<p>WeClone 项目将被部署在该目录下。</p>
<blockquote>
<p>📁 若你希望进一步了解 AutoDL 环境下的目录结构与作用，可参阅官方文档：<a target="_blank" rel="noopener" href="https://www.autodl.com/docs/env/">容器实例概要</a></p>
</blockquote>
<h3 id="Git-版本控制工具"><a href="#Git-版本控制工具" class="headerlink" title="Git 版本控制工具"></a>Git 版本控制工具</h3><p>WeClone 项目的代码托管在 GitHub 上，因此你需要预先安装 <strong>Git</strong> 工具来克隆项目仓库。</p>
<p>如果你使用的是基于 Ubuntu 的系统，可直接运行以下命令进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get update &amp;&amp; apt-get install git</span><br></pre></td></tr></table></figure>

<p>对于其他 Linux 发行版或安装方式，请参考：<br> <a target="_blank" rel="noopener" href="https://www.runoob.com/git/git-install-setup.html">Git 安装与配置教程 - 菜鸟教程</a></p>
<h3 id="加速-GitHub-克隆速度"><a href="#加速-GitHub-克隆速度" class="headerlink" title="加速 GitHub 克隆速度"></a>加速 GitHub 克隆速度</h3><p>在国内访问 GitHub 往往存在网速瓶颈。AutoDL 环境提供了 <strong>学术网络加速工具</strong>，你可以通过以下命令启用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/network_turbo  <span class="comment"># 启用学术加速</span></span><br></pre></td></tr></table></figure>

<p>若后续无需加速，可使用以下命令关闭代理环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unset</span> http_proxy &amp;&amp; <span class="built_in">unset</span> https_proxy</span><br></pre></td></tr></table></figure>

<h3 id="创建虚拟环境并安装-WeClone-依赖"><a href="#创建虚拟环境并安装-WeClone-依赖" class="headerlink" title="创建虚拟环境并安装 WeClone 依赖"></a><strong>创建虚拟环境并安装 WeClone 依赖</strong></h3><p><code>uv</code>是一个由 Astral 开发的快速 Python 包管理器。推荐使用 <code>uv</code> 创建虚拟环境并安装项目依赖，可以避免包版本冲突。</p>
<blockquote>
<p>注：项目尚未提供 <code>requirements.txt</code>，使用其他命令（如 <code>conda</code>）易导致依赖版本冲突。</p>
</blockquote>
<ul>
<li><p><strong>安装 uv (推荐的包管理器)</strong><br>打开命令提示符 (CMD) 或 PowerShell，使用 pip 安装 uv：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p>（直接跳过）<strong>cuda安装</strong>：</p>
<p>由于在创建实例时Auto DL已经为我们配置好了cuda，所以这里不需要再重新安装了。</p>
</li>
<li><p><strong>克隆 WeClone 项目：</strong><br>打开 CMD 或 PowerShell，导航到你希望存放项目的目录，然后克隆仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/xming521/WeClone.git</span><br><span class="line"><span class="built_in">cd</span> WeClone</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>创建并激活虚拟环境 (使用 uv)：</strong></p>
<p>在 <code>WeClone</code> 项目根目录下执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uv venv .venv --python=3.10  <span class="comment"># 你可以指定已安装的 Python 3.10+ 版本</span></span><br><span class="line"><span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure>

<p>激活成功后，你的命令行提示符前通常会显示 <code>(.venv)</code>。</p>
</li>
<li><p><strong>安装项目主要依赖：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install --group main -e .</span><br></pre></td></tr></table></figure>

<p>此命令将读取项目中依赖配置并安装所有库。</p>
</li>
<li><p><strong>（ <code>pytorch</code>安装失败再看）手动安装 <code>pytorch</code></strong></p>
 <details>
   <summary>手动安装 PyTorch 参考教程</summary>
   <p> 网络环境不稳定的情况下安装PyTorch有一定概率会出错，所以可以在环境内安装好 PyTorch。推荐从一些国内镜像源下载好 PyTorch 安装包后在本地离线安装。可以参考下面的教程，但是注意教程中使用的是下载官方包的链接，需要替换成国内镜像源的对应网站。</p>
   <p><strong>参考教程：</strong><a href="https://blog.csdn.net/weixin_44956153/article/details/142303905" target="_blank">PyTorch 离线版本安装教程</a></p>
   安装完后记得重新跑一下`uv pip install --group main -e .`把漏掉的包重新安装上
 </details>

</li>
<li><p><strong>测试 CUDA 环境 (NVIDIA GPU 用户)：</strong><br>安装完依赖后（特别是 PyTorch），运行以下命令测试 CUDA 是否配置正确并能被 PyTorch 识别：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(&#x27;CUDA是否可用:&#x27;, torch.cuda.is_available()); print(&#x27;CUDA版本:&#x27;, torch.version.cuda); print(&#x27;PyTorch版本:&#x27;, torch.__version__)&quot;</span></span><br></pre></td></tr></table></figure>

<p>如果 <code>CUDA是否可用:</code> 显示 <code>True</code>，则表示配置成功。</p>
</li>
<li><p><strong>复制配置文件模板</strong></p>
<p>将配置文件模板复制一份并重命名为<code>settings.jsonc</code>，后续配置修改在此文件进行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> settings.template.jsonc settings.jsonc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>(可选) 安装 FlashAttention：</strong><br>为了加速训练和推理（如果你的硬件支持），可以尝试安装 FlashAttention。</p>
<p>可以直接尝试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install flash-attn --no-build-isolation</span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️Flash Attention 仅适用于 Turing、Ampere、Ada 和 Hopper 架构的 NVIDIA GPU（如 A100、H100、T4、RTX 2080、RTX 3090 等），不支持 Volta 架构的 V100。</p>
</blockquote>
<details>

<summary>如果失败在用下面方法：</summary>

<p><strong>再次检查本地python、torch、cuda的版本</strong>（如果你很清楚你当前的配置可以不用检查）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python --version &amp;&amp;</span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.__version__); print(torch.cuda.is_available())&quot;</span> &amp;&amp;</span><br><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>

<p>执行以上命令，得到你的<code>python</code>、<code>torch</code>和<code>cuda</code>版本。正常来讲你会得到下面的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Python 3.10.12 <span class="comment">#python版本，应该是3.10</span></span><br><span class="line">2.6.0+cu124 <span class="comment">#你的torch版本，安教程来安装的应该是2.6.0</span></span><br><span class="line">True <span class="comment">#CUDA可用</span></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2024 NVIDIA Corporation</span><br><span class="line">Built on Thu_Sep_12_02:18:05_PDT_2024</span><br><span class="line">Cuda compilation tools, release 12.6, V12.6.77</span><br><span class="line">Build cuda_12.6.r12.6/compiler.34841621_0 <span class="comment">#cuda版本，应该大于12.4</span></span><br></pre></td></tr></table></figure>

<p>确定自己的相关版本后，在<a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/releases">Flash-attention下载地址</a>选择对应的<code>whl</code>文件用<code>pip install</code>来安装，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br><span class="line">pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br></pre></td></tr></table></figure>

</details></li>
</ol>
<hr>
<p><strong>到这里，恭喜你完成了全部的环境配置。你已经完成了整个项目部署最难的部分！！！</strong></p>
<blockquote>
<p>💡 <strong>省钱小技巧</strong></p>
<p>在接下来的 <a href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD">模型下载</a> 环节中，通常需要较长等待时间，但此时并不需要使用 GPU。为了节省成本，<strong>AutoDL</strong> 提供了一种 <strong>“无卡模式”开机</strong> 方案，费用仅为 <strong>0.1 元&#x2F;小时</strong>。你可以在“容器实例”管理页面先将当前实例 <strong>关机</strong>，然后使用 <strong>无卡模式重新开机</strong>，即可在无需 GPU 的情况下完成模型下载等任务。</p>
<p>如果你想了解更多细节，可以查看 AutoDL 帮助文档中的 <a target="_blank" rel="noopener" href="https://www.autodl.com/docs/save_money/">省钱绝招</a>。</p>
<p>⚠️ <strong>注意</strong>：关机后如果你希望以 <strong>有卡模式</strong>（即正常方式）重新开机，有可能会遇到 <code>该主机空闲GPU不足</code> 的提示。此时你需要等待 GPU 空闲后，才能继续进行 <a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</a> 及之后依赖 GPU 的步骤。</p>
</blockquote>
<h2 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h2><blockquote>
<p>如果你是使用<strong>无卡模式重新开机</strong>进入的，记得重新进入项目根目录并激活环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="built_in">cd</span> /root/autodl-tmp/WeClone</span><br><span class="line">&gt;<span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure></blockquote>
<p>WeClone 默认使用 Qwen2.5-7B-Instruct 模型，你也可以选择其他你想要使用的模型（不推荐使用深度思考模型）。这里提供两种下载方法：</p>
<h3 id="方法一：使用Git安装（WeClone官方推荐）"><a href="#方法一：使用Git安装（WeClone官方推荐）" class="headerlink" title="方法一：使用Git安装（WeClone官方推荐）"></a><strong>方法一：使用<code>Git</code>安装（WeClone官方推荐）</strong></h3><h4 id="安装-Git-LFS："><a href="#安装-Git-LFS：" class="headerlink" title="安装 Git LFS："></a><strong>安装 Git LFS：</strong></h4><p>在 终端运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt update <span class="comment"># 更新包列表</span></span><br><span class="line">apt install git-lfs <span class="comment"># 安装 git-lfs(ubuntu)</span></span><br><span class="line">git lfs install <span class="comment"># 验证是否安装成功</span></span><br></pre></td></tr></table></figure>
<p>若显示：<code>Git LFS initialized</code>则安装成功。</p>
<blockquote>
<p>每个用户只需执行一次此命令。</p>
</blockquote>
<h4 id="克隆模型仓库："><a href="#克隆模型仓库：" class="headerlink" title="克隆模型仓库："></a><strong>克隆模型仓库：</strong></h4><p>推荐使用 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/models">魔搭社区（ModelScope）</a> 的模型资源，默认下载 <code>Qwen2.5-7B-Instruct</code> 模型。你也可以根据自己的情况和喜好选择该平台的其他模型。</p>
<p>在WeClone根目录下执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git</span><br><span class="line"><span class="comment"># 如果你希望下载完成后自动关机，执行下面这条命令</span></span><br><span class="line">git <span class="built_in">clone</span> https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git &amp;&amp; /usr/bin/shutdown</span><br></pre></td></tr></table></figure>
<blockquote>
<p>⚠️ 注：</p>
<ol>
<li>模型文件较大，下载时间可能较长，请确保磁盘空间充足（建议至少 20GB）。</li>
<li>关于自动关机，用<code>;</code>拼接<code>/usr/bin/shutdown</code>意味着前边的指令不管执行成功与否，都会执行<code>shutdown</code>命令； 用<code>&amp;&amp;</code>拼接表示前边的命令执行成功后才会执行<code>shutdown</code>。请根据自己的需要选择。</li>
</ol>
</blockquote>
<h3 id="方法二：使用ModelScope-命令行工具下载（更快）"><a href="#方法二：使用ModelScope-命令行工具下载（更快）" class="headerlink" title="方法二：使用ModelScope 命令行工具下载（更快）"></a>方法二：使用<code>ModelScope</code> 命令行工具下载（更快）</h3><p>首先安装<code>modelscope</code>库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install modelscope</span><br></pre></td></tr></table></figure>

<p>在项目根目录执行下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model Qwen/Qwen2.5-7B-Instruct --local_dir ./Qwen2.5-7B-Instruct</span><br></pre></td></tr></table></figure>

<p>等待下载完成即可。</p>
<h3 id="修改模型路径或模板"><a href="#修改模型路径或模板" class="headerlink" title="修改模型路径或模板"></a>修改模型路径或模板</h3><p>如果你选择了其他模型、将模型下载到了不同目录或者是使用<code>ModelScope SDK</code>下载的模型，请在 <code>settings.jsonc</code> 中修改路径：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;common_args&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你的模型路径&quot;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;template&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你的模型模板&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<h2 id="使用-PyWxDump-提取微信聊天记录"><a href="#使用-PyWxDump-提取微信聊天记录" class="headerlink" title="使用 PyWxDump 提取微信聊天记录"></a>使用 PyWxDump 提取微信聊天记录</h2><p>要微调模型，首先你需要你的微信聊天数据。</p>
<h3 id="下载并安装PyWxDump："><a href="#下载并安装PyWxDump：" class="headerlink" title="下载并安装PyWxDump："></a><strong>下载并安装PyWxDump：</strong></h3><p><code>PyWxDump</code>是一个用于提取微信聊天记录的工具。由于<code>PyWxDump</code>目前仅确认在Windows环境下正常运行，所以这里切换到你的<strong>本地Windows环境</strong>。</p>
<p><strong>在Windows环境下</strong>访问 <a target="_blank" rel="noopener" href="https://github.com/xaoyaoo/PyWxDump">PyWxDump GitHub 仓库</a> 获取最新版本安装包。安装教程可直接参考<a target="_blank" rel="noopener" href="https://github.com/xaoyaoo/PyWxDump/blob/master/doc/UserGuide.md">PyWxDump官方教程</a></p>
<h3 id="导出数据："><a href="#导出数据：" class="headerlink" title="导出数据："></a><strong>导出数据：</strong></h3><ul>
<li>根据 PyWxDump 的指南，运行软件并解密你的微信数据库</li>
<li>在 PyWxDump 中选择“聊天备份”功能</li>
<li>导出类型选择 CSV</li>
<li>你可以选择导出与多个联系人或群聊的聊天记录（当前版本不建议使用群聊记录）</li>
</ul>
<blockquote>
<p>大数据量的导出如果遇到问题，可以参考官方QQ群里<strong>浮游</strong>大佬的教程（第二章内容）：<a target="_blank" rel="noopener" href="https://img.justlikemaki.vip/file/1747668628253_WeClone%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.html">Windows AI 模型部署与训练指南</a></p>
</blockquote>
<h3 id="整理数据："><a href="#整理数据：" class="headerlink" title="整理数据："></a><strong>整理数据：</strong></h3><ul>
<li><p>PyWxDump 导出的 CSV 文件通常位于其运行目录下的 <code>wxdump_tmp/export</code> 文件夹中。</p>
</li>
<li><p>将整个 <code>csv</code> 文件夹 (其中可能包含多个代表不同聊天对象的子文件夹，每个子文件夹里是对应的聊天记录CSV文件) 压缩为<code>.zip</code>或者<code>.tar</code>格式，然后在刚刚Auto DL的**「容器实例」<strong>页面点击「快捷工具」中的</strong>在线 JupyterLab** 进入JupyterLab。</p>
</li>
</ul>
<blockquote>
<p>（如果你不小心退出了，可以通过<strong>Auto DL主页→控制台→容器实例</strong>返回页面）</p>
</blockquote>
<ul>
<li><p>在JupyterLab依次点击文件夹进入<code>/root/autodl-tmp/WeClone/dataset/</code>目录下，点击上传按钮将压缩包上传至该目录下。（可参考下面的<code>PyWxDump操作及文件上传流程图解</code>）</p>
</li>
<li><p>回到<strong>VS Code</strong>，根据你压缩包的文件类型，在项目根目录执行下面对应命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压zip文件 </span></span><br><span class="line">unzip  /root/autodl-tmp/WeClone/dataset/csv.zip -d /root/autodl-tmp/WeClone/dataset/csv</span><br><span class="line">apt-get update &amp;&amp; apt-get install -y unzip <span class="comment">#如果提示没有zip命令，使用此命令安装</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压tar文件</span></span><br><span class="line">tar -xf/root/autodl-tmp/WeClone/dataset/csv.tar -C /root/autodl-tmp/WeClone/dataset/csv</span><br></pre></td></tr></table></figure>
</li>
<li><p>最终的目录结构应类似于：<code>WeClone/dataset/csv/张三/聊天记录.csv</code> 等。</p>
</li>
</ul>
<details><summary><strong>PyWxDump操作及文件上传流程图解</strong></summary><br>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B0.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B1.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B3.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump%E6%95%99%E7%A8%8B4.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/pywxdump-zip.png" width="90%"/>
</p>
<p align="center">
  <img src="https://blog-img.051088.xyz/linux009.png" width="90%"/>
</p>
</details>



<hr>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>原始的聊天记录需要经过预处理才能用于模型训练。</p>
<ul>
<li><strong>默认处理：</strong> WeClone 项目默认会去除数据中的手机号、身份证号、邮箱和网址。</li>
<li><strong>自定义过滤（可选）：</strong> 项目提供了一个禁用词词库 ，在最新的代码中这个词库被移动到了<code>settings.jsonc</code>的<code>blocked_words</code>。你可以向其中添加不希望出现在训练数据中的词句（包含禁用词的整句会被过滤掉）。</li>
</ul>
<h3 id="执行预处理脚本："><a href="#执行预处理脚本：" class="headerlink" title="执行预处理脚本："></a><strong>执行预处理脚本：</strong></h3><p>在激活虚拟环境的命令行中，进入 WeClone 项目根目录，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli make-dataset  <span class="comment">#在WeClone根目录下执行该命令</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>WeClone</code> 默认启用了 <code>clean_dataset</code> 配置中的 <code>enable_clean</code> 选项，会对原始数据进行清洗，以提升后续处理效果。如果你不打算使用该功能，请将其至为<code>&quot;enable_clean&quot;: false</code>。</p>
</li>
<li><p>当前系统支持使用 <code>llm judge</code> 对聊天记录进行打分，提供 <strong>vllm 离线推理</strong> 和 <strong>API 在线推理</strong> 两种方式。你可以通过将 <code>settings.jsonc</code> 文件中的 <code>&quot;online_llm_clear&quot;: false</code> 修改为 <code>true</code> 来启用 API 在线推理模式，并配置相应的 <code>base_url</code>、<code>llm_api_key</code>、<code>model_name</code> 等参数。所有兼容 OpenAI 接口的模型均可接入，但需注意使用 API 可能带来额外成本。</p>
</li>
<li><p>在获得 <code>llm 打分分数分布情况</code> 后，可通过设置 <code>accept_score</code> 参数筛选可接受的分数区间，同时可适当降低 <code>train_sft_args</code> 中的 <code>lora_dropout</code> 参数，以提升模型的拟合效果。</p>
</li>
<li><p>预处理完成后，数据通常会保存在 <code>\WeClone\dataset\res_csv\sft</code> 目录或其子目录下的 <code>sft-my.json</code> 文件中。</p>
</li>
</ul>
<h3 id="💡-使用-vLLM-时的注意事项"><a href="#💡-使用-vLLM-时的注意事项" class="headerlink" title="💡 使用 vLLM 时的注意事项"></a>💡 使用 vLLM 时的注意事项</h3><p>如果你选择使用<strong>vllm进行离线推理</strong>，且显存有限，需要启用<strong>vLLM的<code>bitsandbytes</code>量化加载</strong>，否则这一步也可能会爆显存。进一步调整、优化<code>vllm</code>参数请查询<a target="_blank" rel="noopener" href="https://docs.vllm.com.cn/en/latest/serving/engine_args.html#engine-args"> vLLM 引擎参数 </a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在下面代码中的engine_args新增参数</span></span><br><span class="line"><span class="comment"># weclone/core/inference/vllm_infer.py</span></span><br><span class="line"></span><br><span class="line">engine_args = &#123;</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model_args.model_name_or_path,</span><br><span class="line">    <span class="string">&quot;trust_remote_code&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&quot;dtype&quot;</span>: model_args.infer_dtype</span><br><span class="line">    <span class="string">&quot;max_model_len&quot;</span>: cutoff_len + max_new_tokens,</span><br><span class="line">    <span class="string">&quot;enable_lora&quot;</span>: model_args.adapter_name_or_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">    <span class="string">&quot;enable_prefix_caching&quot;</span>: <span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ↓ 新增内容 ↓</span></span><br><span class="line">    <span class="string">&quot;quantization&quot;</span>: <span class="string">&quot;bitsandbytes&quot;</span>,</span><br><span class="line">    <span class="string">&quot;load_format&quot;</span>: <span class="string">&quot;bitsandbytes&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[!TIP]<br>如果遇到报错<code>ImportError: Please install bitsandbytes&gt;=0.45.3</code>，可以尝试重新安装<code>bitsandbytes</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;#使用uv安装 bitsandbytes，建议科学上网</span><br><span class="line">&gt;uv pip install bitsandbytes&gt;=0.39.0</span><br></pre></td></tr></table></figure></blockquote>
<ul>
<li><p>此外如果你使用了型号比较老的GPU（例如，计算能力 Compute Capability 低于 8.0 的NVIDIA GPU，如Tesla T4, V100, GTX 10xx&#x2F;20xx系列等）可能会遇到下面报错：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your xxx GPU has compute capability xx. You can use float16 instead by explicitly setting the idtype flag <span class="keyword">in</span> CLI, <span class="keyword">for</span> ecample: --dtype=half.</span><br></pre></td></tr></table></figure>

<p>这是因为： <code>bfloat16</code> (BF16) 是一种较新的浮点数格式，需要GPU硬件达到一定的计算能力（通常是NVIDIA Ampere架构及更新的GPU，计算能力 &gt;&#x3D; 8.0）才能原生支持。如果模型默认尝试以 <code>bfloat16</code> 加载，而你的GPU不支持，就会出现这个错误。这时候你可以尝试在原本的<code>CLI</code>后加上<code>--dtype=half</code>然后重新执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli make-dataset --dtype=half</span><br></pre></td></tr></table></figure>

<p>或者在<code>settings.jsonc</code>中增加以下参数，然后重新执行<code>weclone-cli make-dataset</code>，看问题是否解决。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;infer_args&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1.2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max_length&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">0.65</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;infer_dtype&quot;</span><span class="punctuation">:</span> <span class="string">&quot;float16&quot;</span>  <span class="comment">// 添加这一行</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="修改配置文件-settings-jsonc"><a href="#修改配置文件-settings-jsonc" class="headerlink" title="修改配置文件 (settings.jsonc)"></a>修改配置文件 (<code>settings.jsonc</code>)</h2><p>WeClone 项目的训练与推理核心配置集中在 <code>settings.jsonc</code> 文件中。你需要根据实际使用场景，<strong>适当调整其中的关键参数</strong>。</p>
<p>请打开项目根目录下的 <code>WeClone/settings.jsonc</code> 文件，重点关注以下配置项：</p>
<p><strong>训练相关参数：</strong></p>
<ul>
<li><p><strong><code>per_device_train_batch_size</code></strong> 和 <strong><code>gradient_accumulation_steps</code></strong>：<br> 控制单张显卡的显存占用与有效 batch size。可根据显存大小调整，达到资源利用与训练稳定性的平衡。</p>
<blockquote>
<p><strong>调节建议：</strong></p>
<ul>
<li><strong>显存较小</strong>：减小 <code>per_device_train_batch_size</code>，增大 <code>gradient_accumulation_steps</code>；</li>
<li><strong>显存充足</strong>：可适当增大 <code>per_device_train_batch_size</code>，以加快训练速度；</li>
</ul>
</blockquote>
</li>
<li><p><strong><code>train_sft_args</code> 中的参数</strong>（适用于 SFT 阶段微调）：<br>可根据数据量与任务复杂度调整以下参数：</p>
<ul>
<li><code>num_train_epochs</code>：训练轮数；</li>
<li><code>lora_rank</code>：LoRA 子空间维度，越高越耗显存；</li>
<li><code>lora_dropout</code>：LoRA dropout 比例，用于防止过拟合。</li>
</ul>
</li>
<li><p><strong>建议：</strong></p>
<ul>
<li>配置文件中包含详细注释，建议逐条阅读理解后再做修改。</li>
<li>若你希望使用其他微调策略（如全量微调、Freeze 等），请确保 <code>finetuning_type</code> 与相关参数保持一致。</li>
<li>进一步了解参数，请查看<code>LLaMA Factory</code>官方文档：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html">参数介绍 - LLaMA Factory</a></li>
</ul>
</li>
</ul>
<h3 id="启用-QLoRA-（可选配置）"><a href="#启用-QLoRA-（可选配置）" class="headerlink" title="启用 QLoRA （可选配置）"></a><strong>启用 QLoRA （可选配置）</strong></h3><p>如果你希望进一步减少显存消耗（尤其在显存紧张场景下），可以开启 <strong>QLoRA 量化加速训练</strong>。</p>
<p>在 <code>settings.jsonc</code> 的 <code>common_args</code> 字段中添加以下参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启 QLora</span></span><br><span class="line"><span class="attr">&quot;quantization_bit&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span> <span class="comment">//支持值：2 / 4 / 8，数值越低显存越省，但推理速度和效果可能略有下降。</span></span><br><span class="line"><span class="attr">&quot;quantization_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nf4&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;double_quantization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;quantization_method&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bitsandbytes&quot;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<h3 id="其他加速机制（可选配置）"><a href="#其他加速机制（可选配置）" class="headerlink" title="其他加速机制（可选配置）"></a>其他加速机制（可选配置）</h3><h4 id="Unsloth"><a href="#Unsloth" class="headerlink" title="Unsloth"></a><strong>Unsloth</strong></h4><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth/">Unsloth</a> 框架支持 Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen等大语言模型并且支持 4-bit 和 16-bit 的 QLoRA&#x2F;LoRA 微调，该框架在提高运算速度的同时还<strong>减少了显存占用</strong>。如果你还想进一步<strong>减少显存占用</strong>，可以使用<code>Unsloth</code>加速。</p>
</li>
<li><p>如果想使用 <code>Unsloth</code>, 在 <code>settings.jsonc</code> 的 <code>common_args</code> 字段中添加以下参数：</p>
</li>
</ul>
 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启Unsloth</span></span><br><span class="line"><span class="attr">&quot;use_unsloth&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️安装使用<code>unsloth</code>有可能会导致把<code>transformers</code>升级版本进而引发报错，需要谨慎使用。如果解决不了，请重新执行以下命令，将其降级至之前版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install --group main -e .</span><br></pre></td></tr></table></figure></blockquote>
<h4 id="Liger-Kernel"><a href="#Liger-Kernel" class="headerlink" title="Liger Kernel"></a><strong>Liger Kernel</strong></h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/linkedin/Liger-Kernel/">Liger Kernel</a> 是一个大语言模型训练的性能优化框架, 可有效地<strong>提高吞吐量并减少内存占用</strong>。如果你还想进一步<strong>减少显存占用</strong>，可以使用<code>Unsloth</code>加速。</li>
<li>如果想使用<code> Liger Kernel</code>,在 <code>settings.jsonc</code> 的 <code>common_args</code> 字段中添加以下参数：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;enable_liger_kernel&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意：</strong></p>
<p>在启用QLora、Unsloth等机制后，其他可能会需要额外安装一些项目依赖包中缺失的包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用uv安装依赖</span></span><br><span class="line">例如：</span><br><span class="line">uv pip install bitsandbytes&gt;=0.39.0 <span class="comment">#QLora需要</span></span><br><span class="line">uv pip install unsloth <span class="comment"># unsloth需要</span></span><br></pre></td></tr></table></figure>
<p>如有其他包缺失，也请依次使用<code>uv pip install</code>来安装</p>
</blockquote>
<p><strong>调整建议：</strong></p>
<ul>
<li>配置文件中包含详细注释，建议逐条阅读理解后再做修改。</li>
<li>若你希望使用其他微调策略（如全量微调、Freeze、QLoRA 等），请确保 <code>finetuning_type</code> 与相关参数保持一致。</li>
<li>进一步了解参数详情请查看<code>LLaMA Factory</code>官方文档：<a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html">参数介绍 - LLaMA Factory</a></li>
<li>关于<code>unsloth</code>的报错笔者暂未解决，如想进一步了解请参考<a target="_blank" rel="noopener" href="https://img.justlikemaki.vip/file/1747668628253_WeClone%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.html">Windows AI 模型部署与训练指南</a></li>
</ul>
<hr>
<p><strong>到这里你已经完成所有前期的配置工作了，接下来即将开始正式推理、训练模型！！！</strong></p>
<h2 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h2><p>配置完成后，就可以开始训练了。</p>
<h3 id="单卡训练"><a href="#单卡训练" class="headerlink" title="单卡训练"></a>单卡训练</h3><p>在激活虚拟环境的命令行中，根目录下运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli train-sft</span><br></pre></td></tr></table></figure>

<blockquote>
<p>多卡环境单卡训练，需要先执行 <code>export CUDA_VISIBLE_DEVICES=0</code></p>
</blockquote>
<p>训练脚本会读取 <code>settings.jsonc</code> 中的配置并开始微调。留意终端输出，观察 loss 是否在正常下降。</p>
<h3 id="多卡训练"><a href="#多卡训练" class="headerlink" title="多卡训练"></a>多卡训练</h3><p>如果你有多张 NVIDIA GPU 并希望进行多卡训练：</p>
<ol>
<li><p><strong>安装 Deepspeed：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip install deepspeed</span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️⚠️⚠️注意：<br>Deepspeed 在 Windows 上的原生支持可能有限或配置复杂。官方主要支持 Linux。如果遇到安装或运行问题，可能需要查阅 Deepspeed 官方文档或社区寻求 Windows 解决方案，或者考虑在 WSL2 环境下使用 Deepspeed。</p>
</blockquote>
</li>
<li><p><strong>配置 Deepspeed：</strong><br>在 <code>settings.jsonc</code> 中，找到 <code>deepspeed</code> 配置项，并取消其注释或根据需要填写 Deepspeed 的 JSON 配置文件路径。</p>
</li>
<li><p><strong>启动多卡训练：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --num_gpus=&lt;使用显卡数量&gt; weclone/train/train_sft.py</span><br></pre></td></tr></table></figure>

<p>例如，使用2张显卡：<code>deepspeed --num_gpus=2 weclone/train/train_sft.py</code></p>
</li>
</ol>
<p>训练完成后，微调好的 LoRA 适配器权重会保存在你 <code>settings.jsonc</code> 中指定的 <code>output_dir</code>。</p>
<h2 id="推理-与你的数字分身对话"><a href="#推理-与你的数字分身对话" class="headerlink" title="推理 (与你的数字分身对话)"></a>推理 (与你的数字分身对话)</h2><p>微调完成后，你可以通过以下几种方式与你的数字分身进行交互。</p>
<h3 id="使用浏览器-Demo-简单推理"><a href="#使用浏览器-Demo-简单推理" class="headerlink" title="使用浏览器 Demo 简单推理"></a>使用浏览器 Demo 简单推理</h3><p>这是一种快速测试模型效果并调整推理参数（如 <code>temperature</code>, <code>top_p</code>）的方法。<br>在激活虚拟环境的命令行中，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli webchat-demo</span><br></pre></td></tr></table></figure>

<p>脚本会启动一个服务器本地 Web 服务 (通常在 <code>http://127.0.0.1:7860</code> 或类似地址)，在这里测试出的最佳推理参数可以更新回 <code>settings.jsonc</code> 的 <code>infer_args</code> 部分，供后续使用。</p>
<blockquote>
<p>⚠️注意：这里你还没办法在自己本地浏览器访问 <code>http://127.0.0.1:7860</code> ，需要在进行<a href="#SSH%E4%BB%A3%E7%90%86">SSH代理</a>后才能访问。</p>
</blockquote>
<h3 id="使用-API-接口进行推理"><a href="#使用-API-接口进行推理" class="headerlink" title="使用 API 接口进行推理"></a>使用 API 接口进行推理</h3><p>WeClone 提供了一个 API 服务，可以供其他应用程序调用。</p>
<ol>
<li><p><strong>启动 API 服务：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli server</span><br></pre></td></tr></table></figure>

<p>服务启动后，通常会监听在服务器端的 <code>http://127.0.0.1:8005/v1</code> (具体地址和端口请查看终端输出或 <code>settings.jsonc</code> 中的配置)。</p>
</li>
<li><p><strong>通过 API 调用：</strong><br>你可以使用任何 HTTP客户端 (如 Postman, curl，或 Python 的 <code>requests</code> 库) 向该 API 发送请求。API 通常兼容 OpenAI 的格式。同时也可以将其<a href="#%E9%83%A8%E7%BD%B2%E5%88%B0%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA">部署到聊天机器人</a>。</p>
</li>
</ol>
<h3 id="量化与本地部署（可选）"><a href="#量化与本地部署（可选）" class="headerlink" title="量化与本地部署（可选）"></a><strong>量化与本地部署（可选）</strong></h3><p>可以考虑使用 <code>llama.app</code> 等工具对训练好的模型进行量化处理，并导出<code>gguf</code>模型，在本地实现高效推理。</p>
<p>这里可以参考官方QQ群里<strong>浮游</strong>大佬的教程（第四章内容）：<a target="_blank" rel="noopener" href="https://img.justlikemaki.vip/file/1747668628253_WeClone%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.html">Windows AI 模型部署与训练指南</a></p>
<h3 id="使用常见聊天问题测试"><a href="#使用常见聊天问题测试" class="headerlink" title="使用常见聊天问题测试"></a>使用常见聊天问题测试</h3><p>项目还提供了一个脚本，可以使用预设的问题列表来测试模型。</p>
<ol>
<li>确保 API 服务 (<code>weclone-cli server</code>) 正在运行。</li>
<li>打开一个新的命令行窗口 (并激活虚拟环境)，然后运行：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weclone-cli test-model</span><br></pre></td></tr></table></figure>
测试结果会输出到 <code>test_result-my.txt</code>。</li>
</ol>
<h2 id="SSH代理"><a href="#SSH代理" class="headerlink" title="SSH代理"></a><strong>SSH代理</strong></h2><p>由于 <strong>AutoDL 实例不具备独立公网 IP</strong>，无法直接开放端口给外部访问。因此，我们需要通过 <strong>SSH 隧道</strong> 实现端口转发：将远程实例中的端口代理到本地，或反向将本地端口代理至实例。否则你将无法从本地计算机调用远程服务接口（如 API）。</p>
<p>AutoDL 提供了两种主流代理方式：</p>
<ul>
<li>图形界面工具（适用于 <strong>Windows</strong> 用户）</li>
<li>SSH 命令行代理（适用于 <strong>Linux &#x2F; macOS</strong> 用户）</li>
</ul>
<h3 id="方法一：图形界面工具（Windows-专用）"><a href="#方法一：图形界面工具（Windows-专用）" class="headerlink" title="方法一：图形界面工具（Windows 专用）"></a><strong>方法一：<a target="_blank" rel="noopener" href="https://www.autodl.com/docs/ssh_proxy/#_1">图形界面工具</a>（Windows 专用）</strong></h3><p>下载并解压 <a target="_blank" rel="noopener" href="https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/AutoDL-SSH-Tools.zip">AutoDL Windows 端口代理工具</a>，双击运行其中的 <code>.exe</code> 文件，无需安装。在工具界面中，只需填写要代理的端口号，即可轻松将远程端口映射到本地，或反向映射。</p>
<p>对于<strong>WeClone项目</strong>，你可以选择将<strong>webchat-demo</strong>对应的<code>7860</code>端口或者<strong>server</strong>对应的<code>8005</code>端口代理到本地。即在下图的<strong>代理到本地端口</strong>栏填写<code>7860</code>或者<code>8005</code>。</p>
<blockquote>
<p>⚠️注意： 如需同时代理多个端口，请使用<strong>英文逗号</strong>分隔：<code>7860,8005</code></p>
</blockquote>
<img src="https://www.autodl.com/docs/assets/2024-10-17-19-11-33-image.png" alt="img" style="zoom:50%;" />

<h3 id="方法二：使用-SSH-命令代理"><a href="#方法二：使用-SSH-命令代理" class="headerlink" title="方法二：使用 SSH 命令代理"></a><strong>方法二：<a target="_blank" rel="noopener" href="https://www.autodl.com/docs/ssh_proxy/#ssh_1">使用 SSH 命令代理</a></strong></h3><p>适用于 Linux、macOS 以及熟悉终端操作的 Windows 用户。</p>
<ol>
<li><p>在 AutoDL 实例中启动所需服务（如 <code>webchat-demo</code> 或 <code>server</code>）。<br> 这里以 <code>webchat-demo</code> 所使用的 <code>7860</code> 端口为例，如果要代理的是 <code>server</code>，只需将端口号改为 <code>8005</code>。</p>
</li>
<li><p>在<strong>本地电脑</strong>的终端(cmd &#x2F; powershell &#x2F; terminal等)中执行代理命令：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -CNg -L 7860:127.0.0.1:7860 root@123.125.240.150 -p 42151</span><br></pre></td></tr></table></figure>

<p> 其中<code>root@123.125.240.150</code>和<code>42151</code>分别是实例中SSH指令的访问地址与端口，请找到自己实例的ssh指令<strong>做相应替换</strong>。<code>7860:127.0.0.1:7860</code>是指代理实例内<code>7860</code>端口到本地的<code>7860</code>端口。</p>
</li>
</ol>
<blockquote>
<p>注意：执行完这条ssh命令，没有任何日志是正常的，只要没有要求重新输入密码或错误退出</p>
<p><strong>Windows下的cmd&#x2F;powershell如果一直提示密码错误，是因为无法粘贴，手动输入即可（正常不会显示正在输入的密码)</strong></p>
</blockquote>
<ol start="3">
<li>在本地浏览器中访问<code>http://127.0.0.1:7860</code>即可打开你部署的 <code>webchat-demo</code> 服务，注意这里的<code>7860</code>端口要和上述<code>7860:127.0.0.1:7860</code>中的端口保持一致。</li>
</ol>
<blockquote>
<p>💡 <strong>补充：</strong></p>
<p> <code>server</code>服务是无法通过浏览器访问的，其端口用于<a href="#%E9%83%A8%E7%BD%B2%E5%88%B0%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA">部署到聊天机器人</a>或其他<code>api</code>服务。</p>
<p><code>http://127.0.0.1</code>和下文提到的<code>http://localhost</code>含义相同，都是本地回环地址。填写两者任意一种均可。</p>
<p>如果你还有其他问题，请访问<a target="_blank" rel="noopener" href="https://www.autodl.com/docs/ssh_proxy/#_2">AutoDL帮助文档</a>查看。</p>
</blockquote>
<h2 id="部署到聊天机器人"><a href="#部署到聊天机器人" class="headerlink" title="部署到聊天机器人"></a>部署到聊天机器人</h2><p>你可以通过 <strong>AstrBot</strong> 或 <strong>LangBot</strong> 将<code>server</code>服务部署到本地聊天机器人。接下来的操作需要在<strong>本地计算机</strong>完成。</p>
<h3 id="通过-AstrBot-部署"><a href="#通过-AstrBot-部署" class="headerlink" title="通过 AstrBot 部署"></a>通过 AstrBot 部署</h3><details>
    <summary>通过 AstrBot 部署到聊天机器人操作流程</summary>
    [AstrBot](https://github.com/AstrBotDevs/AstrBot) 是一个支持多平台的 LLM 聊天机器人框架，可以将你的 WeClone 模型部署到 QQ、微信、Telegram 等平台。

<ol>
<li><p><strong>部署 AstrBot：</strong> 根据 AstrBot 官方文档的指引，在你的服务器或本地安装并配置好 AstrBot。</p>
<p> <a target="_blank" rel="noopener" href="https://astrbot.app/deploy/astrbot/windows.html">使用 Windows 一键安装器部署 AstrBot | AstrBot</a></p>
</li>
<li><p><strong>启动 WeClone API 服务：</strong> 确保你的 <code>weclone-cli server</code> 正在运行，并且 AstrBot 可以访问到该服务的地址和端口。</p>
</li>
<li><p><strong>在 AstrBot 中新增服务提供商：</strong></p>
<ul>
<li>启动AstrBot ，并进入其web UI界面，一般是<code>http://localhost:6185</code>。</li>
<li>依次点击<strong>服务提供商</strong>→<strong>新增服务提供商</strong></li>
<li>类型选择：OpenAI</li>
<li>API Base URL：填写你的 WeClone API 服务地址 (如果部署在本地则填写 <code>http://localhost:8005/v1</code>)。</li>
<li>模型：可以填写一个占位符，如 <code>gpt-3.5-turbo</code> (因为实际使用的模型由 WeClone API 服务决定)。</li>
<li>API Key：随意填写一个即可（填写完记得回车或者点击右侧添加按钮）</li>
<li>完成后点击“启用”和“保存”等待AstrBot 重启。</li>
</ul>
  <img src="https://blog-img.051088.xyz/AstrBo02.png" style="zoom:70%;" />
</li>
<li><p><strong>在 AstrBot 中部署消息平台：</strong> 在“消息平台”界面配置 AstrBot ，以连接到你希望使用的聊天平台 (如微信、QQ 等)。不同消息平台应参照 <a target="_blank" rel="noopener" href="https://astrbot.app/">AstrBot官方文档</a> 。例如如果你想要部署到QQ，可以参考下面AstrBot提供的教程：<a target="_blank" rel="noopener" href="https://astrbot.app/deploy/platform/aiocqhttp/napcat.html#%E9%80%9A%E8%BF%87-napcatqq-%E5%8D%8F%E8%AE%AE%E5%AE%9E%E7%8E%B0%E7%AB%AF%E6%8E%A5%E5%85%A5-qq">通过 NapCatQQ 协议实现端接入 QQ | AstrBot</a></p>
</li>
<li><p><strong>关闭工具调用 (重要)：</strong><br>微调后的模型主要用于模仿你的语言风格，通常不支持复杂的工具调用。在 AstrBot 对应的聊天平台中，向你的机器人发送指令关闭所有默认工具，以确保能看到微调效果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/tool off all</span><br></pre></td></tr></table></figure>

<p>你还可以在“插件管理”页面手动关闭系统插件。这里我只保留了<code>astrbot</code>、<code>session_controller</code>两个系统插件。</p>
<p><img src="https://blog-img.051088.xyz/AstrBo03.png"></p>
</li>
<li><p><strong>设置系统提示词：</strong><br>在 AstrBot 的配置中，为你的机器人设置<strong>系统提示词 (System Prompt)</strong>。这个提示词必须与你微调模型时在 <code>settings.jsonc</code> 中设置的 <code>default_system</code> <strong>完全一致</strong>。<img src="https://blog-img.051088.xyz/AstrBot%E6%95%99%E7%A8%8B01.png"/></p>
</li>
<li><p><strong>其他功能</strong></p>
<p>在“配置文件”界面还有其他很多配置，可以查询 <a target="_blank" rel="noopener" href="https://astrbot.app/">AstrBot官方文档</a> 自行调节。</p>
</li>
<li><p><strong>调整采样参数：</strong><br>根据你在浏览器 Demo 或其他测试中得到的最佳效果，在 AstrBot 中调整模型的采样参数，如 <code>temperature</code>, <code>top_p</code>, <code>top_k</code> 等。具体配置方法请参考 AstrBot 文档中关于<a target="_blank" rel="noopener" href="https://astrbot.app/config/model-config.html">配置自定义的模型参数 | AstrBot</a>的部分。</p>
</li>
</ol>
<blockquote>
<p>⚠️<br>经常检查 <code>api_service.py</code> 的日志输出，确保 AstrBot 发送给大模型服务的请求参数 (如 system prompt, temperature 等) 与你微调和测试时期望的一致。</p>
</blockquote>
</details>



<h3 id="通过-LangBot-部署"><a href="#通过-LangBot-部署" class="headerlink" title="通过 LangBot 部署"></a>通过 LangBot 部署</h3><details>

<summary>通过 LangBot 部署到聊天机器人操作流程</summary>

<p><a target="_blank" rel="noopener" href="https://github.com/RockChinQ/LangBot">LangBot</a> 是一个开源的接入全球多种即时通信平台的 LLM 机器人平台，适合各种场景使用。将WeClone服务端接入LangBot的流程与接入AstrBot类似。下面是流程简介：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8">部署 LangBot</a></li>
<li>在 LangBot 中添加一个机器人</li>
<li>在模型页添加新模型，名称<code>gpt-3.5-turbo</code>，供应商选择 OpenAI，填写 请求 URL 为 WeClone 的地址，详细连接方式可以参考<a target="_blank" rel="noopener" href="https://docs.langbot.app/zh/workshop/network-details.html">文档</a>，API Key 任意填写。</li>
</ol>
<img width="400px" alt="image" src="https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba" />

<ol start="6">
<li>在流水线配置中选择刚才添加的模型，或修改提示词配置</li>
</ol>
<img width="400px" alt="image" src="https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e" />

</details>

<hr>
<p><strong>现在，你的专属数字分身应该已经成功部署到聊天机器人平台了！快去和“它”聊聊天，看看效果如何吧。</strong></p>
<blockquote>
<p>💡温馨提示：服务器用完记得关机！！！</p>
</blockquote>
<h2 id="问题解决与支持"><a href="#问题解决与支持" class="headerlink" title="问题解决与支持"></a>问题解决与支持</h2><ul>
<li><p><strong>微调问题：</strong> 可以参考 LLaMA Factory 的 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory/issues/4614">FAQs | 常见问题</a>，因为 WeClone 底层使用了 LLaMA Factory 的部分组件或类似逻辑。</p>
</li>
<li><p><strong>项目 Issues：</strong> 查看 WeClone GitHub 仓库的 <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone/issues">Issues</a> 和 <a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone/discussions">Discussions</a> 是否有类似问题和解决方案。</p>
</li>
<li><div style="display: flex; align-items: up; gap: 8px;">
  <span style="font-weight: bold;">项目官方群聊：</span>
  <a href="https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&jump_from=webapi&authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn" target="_blank">
    <img src="https://img.shields.io/badge/QQ群-708067078-12B7F5?style=for-the-badge&logo=tencentqq&logoColor=white" alt="QQ群">
  </a>
  <a href="https://t.me/+JEdak4m0XEQ3NGNl" target="_blank">
    <img src="https://img.shields.io/badge/Telegram-WeClone-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white" alt="Telegram">
  </a>
</div></li>
</ul>
<h2 id="免责声明"><a href="#免责声明" class="headerlink" title="免责声明"></a>免责声明</h2><blockquote>
<p>⚠️ ⚠️ ⚠️<br>请勿用于非法用途，否则后果自负。<br>本教程仅供学习交流使用。任何违反法律法规、侵犯他人合法权益的行为，均与WeClone项目及笔者无关。严禁用于窃取他人隐私。</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/xming521/WeClone?tab=readme-ov-file">xming521&#x2F;WeClone – 从聊天记录创造数字分身的一站式解决方案</a>：项目主页，涵盖聊天记录处理、LoRA 微调、声音克隆等模块，本文主要基于项目README编写。</li>
<li><a target="_blank" rel="noopener" href="https://img.justlikemaki.vip/file/1747668628253_WeClone%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.html">Windows AI 模型部署与训练指南</a>：<strong>浮游</strong>大佬撰写的对Weclone部署的避坑指南，本文在编写中参考了很多部分内容。</li>
</ul>
<blockquote>
<p>其余引用链接已在正文中明确标注。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.051088.xyz/">MarkBai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.051088.xyz/posts/weclone-linux-tutorial/">https://blog.051088.xyz/posts/weclone-linux-tutorial/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.051088.xyz" target="_blank">MarkBai的数字自留地</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/WeClone/">WeClone</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/">数字分身</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大语言模型</a><a class="post-meta__tags" href="/tags/%E6%8C%87%E5%8D%97/">指南</a><a class="post-meta__tags" href="/tags/Linux/">Linux</a></div><div class="post-share"><div class="social-share" data-image="https://blog-img.051088.xyz/weclone.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身"><img class="cover" src="https://blog-img.051088.xyz/weclone.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">WeClone — 用微信聊天记录打造自己的 AI 数字分身</div></div><div class="info-2"><div class="info-item-1">WeClone：让你的聊天记录拥有“灵魂”你是否曾设想过——如果你的聊天风格、口头禅，甚至独特的表达习惯，能够被 AI 学会，并在数字世界中“复刻”一个你，会是怎样的体验？ xming521&#x2F;WeClone 项目 正是在这样一个愿景下诞生的尝试。 WeClone 是一个一站式解决方案，它通过分析你的微信聊天记录，微调大语言模型（LLM），让模型说出“你那味儿”的话，并将其接入聊天机器人，打造出专属于你的数字分身。         点击展开查看训练效果图                                                然而，WeClone 官方并未对 Windows 环境进行系统测试，因此许多 Windows 用户在安装与运行过程中可能会遇到各种问题。此外，项目中提到的 QLoRA 微调流程在官方文档中讲解较为简略，对新手用户不够友好。 这篇博客将作为一份补充指南，专门面向 Windows 用户，介绍如何在本地从零搭建和运行 WeClone，并一步步训练出属于你自己的数字克隆。  ⚠️重要提示 WeClone...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身"><img class="cover" src="https://blog-img.051088.xyz/weclone.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-14</div><div class="info-item-2">WeClone — 用微信聊天记录打造自己的 AI 数字分身</div></div><div class="info-2"><div class="info-item-1">WeClone：让你的聊天记录拥有“灵魂”你是否曾设想过——如果你的聊天风格、口头禅，甚至独特的表达习惯，能够被 AI 学会，并在数字世界中“复刻”一个你，会是怎样的体验？ xming521&#x2F;WeClone 项目 正是在这样一个愿景下诞生的尝试。 WeClone 是一个一站式解决方案，它通过分析你的微信聊天记录，微调大语言模型（LLM），让模型说出“你那味儿”的话，并将其接入聊天机器人，打造出专属于你的数字分身。         点击展开查看训练效果图                                                然而，WeClone 官方并未对 Windows 环境进行系统测试，因此许多 Windows 用户在安装与运行过程中可能会遇到各种问题。此外，项目中提到的 QLoRA 微调流程在官方文档中讲解较为简略，对新手用户不够友好。 这篇博客将作为一份补充指南，专门面向 Windows 用户，介绍如何在本地从零搭建和运行 WeClone，并一步步训练出属于你自己的数字克隆。  ⚠️重要提示 WeClone...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/profile-photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">MarkBai</div><div class="author-info-description">技术博客，记录计算机、AI、单片机与结构工程领域的折腾与探索
</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/BAIKEMARK"><i class="fab fa-github"></i><span>My Github</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/BAIKEMARK" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:blog@051088.xyz" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://space.bilibili.com/490783536?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="fa-brands fa-bilibili" style="color: #00a5dc;"></i></a><a class="social-icon" href="https://www.xiaohongshu.com/user/profile/607994f60000000001007812" target="_blank" title="小红书"><i class="fab fa-xiaohongshu" style="color: #ff3b30;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">WeClone-Linux保姆级教程上新啦！！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#WeClone%EF%BC%9A%E8%AE%A9%E4%BD%A0%E7%9A%84%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%8B%A5%E6%9C%89%E2%80%9C%E7%81%B5%E9%AD%82%E2%80%9D"><span class="toc-number">1.</span> <span class="toc-text">WeClone：让你的聊天记录拥有“灵魂”</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%EF%BC%88%E5%BA%9F%E8%AF%9D%E7%89%88%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">简介（废话版）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A7%9F%E5%80%9F%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">租借云服务器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E9%80%89%E6%8B%A9"><span class="toc-number">4.</span> <span class="toc-text">服务器配置选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B4%AD%E4%B9%B0%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">服务器购买与初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">6.</span> <span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">6.1.</span> <span class="toc-text">连接服务器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Git-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7"><span class="toc-number">6.2.</span> <span class="toc-text">Git 版本控制工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E9%80%9F-GitHub-%E5%85%8B%E9%9A%86%E9%80%9F%E5%BA%A6"><span class="toc-number">6.3.</span> <span class="toc-text">加速 GitHub 克隆速度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%B9%B6%E5%AE%89%E8%A3%85-WeClone-%E4%BE%9D%E8%B5%96"><span class="toc-number">6.4.</span> <span class="toc-text">创建虚拟环境并安装 WeClone 依赖</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"><span class="toc-number">7.</span> <span class="toc-text">模型下载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E4%BD%BF%E7%94%A8Git%E5%AE%89%E8%A3%85%EF%BC%88WeClone%E5%AE%98%E6%96%B9%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">方法一：使用Git安装（WeClone官方推荐）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Git-LFS%EF%BC%9A"><span class="toc-number">7.1.1.</span> <span class="toc-text">安装 Git LFS：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%8B%E9%9A%86%E6%A8%A1%E5%9E%8B%E4%BB%93%E5%BA%93%EF%BC%9A"><span class="toc-number">7.1.2.</span> <span class="toc-text">克隆模型仓库：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8ModelScope-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E4%B8%8B%E8%BD%BD%EF%BC%88%E6%9B%B4%E5%BF%AB%EF%BC%89"><span class="toc-number">7.2.</span> <span class="toc-text">方法二：使用ModelScope 命令行工具下载（更快）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E8%B7%AF%E5%BE%84%E6%88%96%E6%A8%A1%E6%9D%BF"><span class="toc-number">7.3.</span> <span class="toc-text">修改模型路径或模板</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-PyWxDump-%E6%8F%90%E5%8F%96%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95"><span class="toc-number">8.</span> <span class="toc-text">使用 PyWxDump 提取微信聊天记录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85PyWxDump%EF%BC%9A"><span class="toc-number">8.1.</span> <span class="toc-text">下载并安装PyWxDump：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-number">8.2.</span> <span class="toc-text">导出数据：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E7%90%86%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-number">8.3.</span> <span class="toc-text">整理数据：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">9.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E9%A2%84%E5%A4%84%E7%90%86%E8%84%9A%E6%9C%AC%EF%BC%9A"><span class="toc-number">9.1.</span> <span class="toc-text">执行预处理脚本：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%92%A1-%E4%BD%BF%E7%94%A8-vLLM-%E6%97%B6%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">9.2.</span> <span class="toc-text">💡 使用 vLLM 时的注意事项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-settings-jsonc"><span class="toc-number">10.</span> <span class="toc-text">修改配置文件 (settings.jsonc)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E7%94%A8-QLoRA-%EF%BC%88%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%EF%BC%89"><span class="toc-number">10.1.</span> <span class="toc-text">启用 QLoRA （可选配置）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%88%B6%EF%BC%88%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%EF%BC%89"><span class="toc-number">10.2.</span> <span class="toc-text">其他加速机制（可选配置）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Unsloth"><span class="toc-number">10.2.1.</span> <span class="toc-text">Unsloth</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Liger-Kernel"><span class="toc-number">10.2.2.</span> <span class="toc-text">Liger Kernel</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">11.</span> <span class="toc-text">微调模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="toc-number">11.1.</span> <span class="toc-text">单卡训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="toc-number">11.2.</span> <span class="toc-text">多卡训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E7%90%86-%E4%B8%8E%E4%BD%A0%E7%9A%84%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB%E5%AF%B9%E8%AF%9D"><span class="toc-number">12.</span> <span class="toc-text">推理 (与你的数字分身对话)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%B5%8F%E8%A7%88%E5%99%A8-Demo-%E7%AE%80%E5%8D%95%E6%8E%A8%E7%90%86"><span class="toc-number">12.1.</span> <span class="toc-text">使用浏览器 Demo 简单推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-API-%E6%8E%A5%E5%8F%A3%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="toc-number">12.2.</span> <span class="toc-text">使用 API 接口进行推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E4%B8%8E%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">12.3.</span> <span class="toc-text">量化与本地部署（可选）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B8%B8%E8%A7%81%E8%81%8A%E5%A4%A9%E9%97%AE%E9%A2%98%E6%B5%8B%E8%AF%95"><span class="toc-number">12.4.</span> <span class="toc-text">使用常见聊天问题测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSH%E4%BB%A3%E7%90%86"><span class="toc-number">13.</span> <span class="toc-text">SSH代理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E5%B7%A5%E5%85%B7%EF%BC%88Windows-%E4%B8%93%E7%94%A8%EF%BC%89"><span class="toc-number">13.1.</span> <span class="toc-text">方法一：图形界面工具（Windows 专用）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8-SSH-%E5%91%BD%E4%BB%A4%E4%BB%A3%E7%90%86"><span class="toc-number">13.2.</span> <span class="toc-text">方法二：使用 SSH 命令代理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E5%88%B0%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA"><span class="toc-number">14.</span> <span class="toc-text">部署到聊天机器人</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87-AstrBot-%E9%83%A8%E7%BD%B2"><span class="toc-number">14.1.</span> <span class="toc-text">通过 AstrBot 部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87-LangBot-%E9%83%A8%E7%BD%B2"><span class="toc-number">14.2.</span> <span class="toc-text">通过 LangBot 部署</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E4%B8%8E%E6%94%AF%E6%8C%81"><span class="toc-number">15.</span> <span class="toc-text">问题解决与支持</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E"><span class="toc-number">16.</span> <span class="toc-text">免责声明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">17.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/weclone-linux-tutorial/" title="WeClone：基于 Linux 的部署指南【保姆级教程】"><img src="https://blog-img.051088.xyz/weclone.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WeClone：基于 Linux 的部署指南【保姆级教程】"/></a><div class="content"><a class="title" href="/posts/weclone-linux-tutorial/" title="WeClone：基于 Linux 的部署指南【保姆级教程】">WeClone：基于 Linux 的部署指南【保姆级教程】</a><time datetime="2025-05-22T09:00:00.000Z" title="发表于 2025-05-22 17:00:00">2025-05-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身"><img src="https://blog-img.051088.xyz/weclone.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WeClone — 用微信聊天记录打造自己的 AI 数字分身"/></a><div class="content"><a class="title" href="/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/" title="WeClone — 用微信聊天记录打造自己的 AI 数字分身">WeClone — 用微信聊天记录打造自己的 AI 数字分身</a><time datetime="2025-05-14T14:00:00.000Z" title="发表于 2025-05-14 22:00:00">2025-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客"><img src="https://blog-img.051088.xyz/2009-2024-08-23043351-1724402031271.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="我的第一篇博客"/></a><div class="content"><a class="title" href="/2025/04/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客">我的第一篇博客</a><time datetime="2025-04-19T12:30:00.000Z" title="发表于 2025-04-19 20:30:00">2025-04-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://blog-img.051088.xyz/wallhaven-5gwvz5_1920x1080.png);"><div id="footer-wrap"><div class="copyright">&copy;2025 By MarkBai</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">Hi, welcome to <a href="https://blog.051088.xyz/">MarkBai's Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-twikoo.051088.xyz',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://blog-twikoo.051088.xyz',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script id="canvas_nest" defer="defer" color="100,200,255" opacity="0.7" zIndex="-1" count="66" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="GOOD,LUCK,FOR,YOU" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>